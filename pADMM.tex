%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2014 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{calc}
\usepackage[T1]{fontenc}
\usepackage{xspace}
\usepackage{amsmath,amssymb}
\usepackage{amsthm,amscd,amsfonts}
\usepackage{bm}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2014}

\newenvironment{packed_enum}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}

\def\x{{\bm x}}
\def\z{{\bm z}}
\def\y{{\bm y}}
\def\s{{\bm s}}
\def\v{{\bm v}}
\def\n{{\bm n}}
\def\L{{\cal L}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\st}{subject~to}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\beqs}{\begin{eqnarray}}
\newcommand{\eeqs}{\end{eqnarray}}
\newcommand{\barr}{\begin{array}}
\newcommand{\earr}{\end{array}}

\newcommand{\Rc}[0]{\ensuremath{\mathcal{R}}\xspace}
\newcommand{\Nc}[0]{\ensuremath{\mathcal{N}}\xspace}
\newcommand{\Gc}[0]{\ensuremath{\mathcal{G}}\xspace}
\newcommand{\Dc}[0]{\ensuremath{\mathcal{D}}\xspace}
\newcommand{\Oc}[0]{\ensuremath{\mathcal{O}}\xspace}
\newcommand{\Wc}[0]{\ensuremath{\mathcal{W}}\xspace}
\newcommand{\E}[0]{\ensuremath{\mathbb{E}}\xspace}

\newcommand{\zerov}[0]{\ensuremath{{\bf 0}}\xspace}
\newcommand{\onev}[0]{\ensuremath{{\bf 1}}\xspace}

\newcommand{\ie}[0]{\emph{i.e., }}
\newcommand{\ea}[0]{\emph{et al. }}
\newcommand{\eg}[0]{\emph{e.g., }}
\newcommand{\cf}[0]{\emph{cf. }}
\newcommand{\etc}[0]{\emph{etc.}}

\newcommand{\Amat}[0]{\ensuremath{{\bf A}}\xspace}
\newcommand{\Bmat}[0]{\ensuremath{{\bf B}}\xspace}
\newcommand{\Cmat}[0]{\ensuremath{{\bf C}}\xspace}
\newcommand{\Dmat}[0]{\ensuremath{{\bf D}}\xspace}
\newcommand{\Emat}[0]{\ensuremath{{\bf E}}\xspace}
\newcommand{\Fmat}[0]{\ensuremath{{\bf F}}\xspace}
\newcommand{\Gmat}[0]{\ensuremath{{\bf G}}\xspace}
\newcommand{\Hmat}[0]{\ensuremath{{\bf H}}\xspace}
\newcommand{\Imat}[0]{\ensuremath{{\bf I}}\xspace}
\newcommand{\Jmat}[0]{\ensuremath{{\bf J}}\xspace}
\newcommand{\Kmat}[0]{\ensuremath{{\bf K}}\xspace}
\newcommand{\Lmat}[0]{\ensuremath{{\bf L}}\xspace}
\newcommand{\Mmat}[0]{\ensuremath{{\bf M}}\xspace}
\newcommand{\Nmat}[0]{\ensuremath{{\bf N}}\xspace}
\newcommand{\Omat}[0]{\ensuremath{{\bf O}}\xspace}
\newcommand{\Pmat}[0]{\ensuremath{{\bf P}}\xspace}
\newcommand{\Qmat}[0]{\ensuremath{{\bf Q}}\xspace}
\newcommand{\Rmat}[0]{\ensuremath{{\bf R}}\xspace}
\newcommand{\Smat}[0]{\ensuremath{{\bf S}}\xspace}
\newcommand{\Tmat}[0]{\ensuremath{{\bf T}}\xspace}
\newcommand{\Umat}[0]{\ensuremath{{\bf U}}\xspace}
\newcommand{\Vmat}[0]{\ensuremath{{\bf V}}\xspace}
\newcommand{\Wmat}[0]{\ensuremath{{\bf W}}\xspace}
\newcommand{\Xmat}[0]{\ensuremath{{\bf X}}\xspace}
\newcommand{\Ymat}[0]{\ensuremath{{\bf Y}}\xspace}
\newcommand{\Zmat}[0]{\ensuremath{{\bf Z}}\xspace}

\newcommand{\1}[0]{\ensuremath{\boldsymbol{1}}\xspace}
\newcommand{\av}[0]{\ensuremath{\boldsymbol{a}}\xspace}
\newcommand{\bv}[0]{\ensuremath{\boldsymbol{b}}\xspace}
\newcommand{\cv}[0]{\ensuremath{\boldsymbol{c}}\xspace}
\newcommand{\dv}[0]{\ensuremath{\boldsymbol{d}}\xspace}
\newcommand{\ev}[0]{\ensuremath{\boldsymbol{e}}\xspace}
\newcommand{\fv}[0]{\ensuremath{\boldsymbol{f}}\xspace}
\newcommand{\gv}[0]{\ensuremath{\boldsymbol{g}}\xspace}
\newcommand{\hv}[0]{\ensuremath{\boldsymbol{h}}\xspace}
\newcommand{\iv}[0]{\ensuremath{\boldsymbol{i}}\xspace}
\newcommand{\jv}[0]{\ensuremath{\boldsymbol{j}}\xspace}
\newcommand{\kv}[0]{\ensuremath{\boldsymbol{k}}\xspace}
\newcommand{\lv}[0]{\ensuremath{\boldsymbol{l}}\xspace}
\newcommand{\mv}[0]{\ensuremath{\boldsymbol{m}}\xspace}
\newcommand{\nv}[0]{\ensuremath{\boldsymbol{n}}\xspace}
\newcommand{\ov}[0]{\ensuremath{\boldsymbol{o}}\xspace}
\newcommand{\pv}[0]{\ensuremath{\boldsymbol{p}}\xspace}
\newcommand{\qv}[0]{\ensuremath{\boldsymbol{q}}\xspace}
\newcommand{\rv}[0]{\ensuremath{\boldsymbol{r}}\xspace}
\newcommand{\sv}[0]{\ensuremath{\boldsymbol{s}}\xspace}
\newcommand{\tv}[0]{\ensuremath{\boldsymbol{t}}\xspace}
\newcommand{\uv}[0]{\ensuremath{\boldsymbol{u}}\xspace}
\newcommand{\vv}[0]{\ensuremath{\boldsymbol{v}}\xspace}
\newcommand{\wv}[0]{\ensuremath{\boldsymbol{w}}\xspace}
\newcommand{\xv}[0]{\ensuremath{\boldsymbol{x}}\xspace}
\newcommand{\yv}[0]{\ensuremath{\boldsymbol{y}}\xspace}
\newcommand{\zv}[0]{\ensuremath{\boldsymbol{z}}\xspace}

\newcommand{\Gammamat}[0]{\ensuremath{\boldsymbol{\Gamma}}\xspace}
\newcommand{\Deltamat}[0]{\ensuremath{\boldsymbol{\Delta}}\xspace}
\newcommand{\Thetamat}[0]{\ensuremath{\boldsymbol{\Theta}}\xspace}
\newcommand{\Lambdamat}[0]{\ensuremath{\boldsymbol{\Lambda}}\xspace}
\newcommand{\Ximat}[0]{\ensuremath{\boldsymbol{\Xi}}\xspace}
\newcommand{\Pimat}[0]{\ensuremath{\boldsymbol{\Pi}}\xspace}
\newcommand{\Sigmamat}[0]{\ensuremath{\boldsymbol{\Sigma}}\xspace}
\newcommand{\Upsilonmat}[0]{\ensuremath{\boldsymbol{\Upsilon}}\xspace}
\newcommand{\Phimat}[0]{\ensuremath{\boldsymbol{\Phi}}\xspace}
\newcommand{\Psimat}[0]{\ensuremath{\boldsymbol{\Psi}}\xspace}
\newcommand{\Omegamat}[0]{\ensuremath{\boldsymbol{\Omega}}\xspace}

\newcommand{\alphav}[0]{\ensuremath{\boldsymbol{\alpha}}\xspace}
\newcommand{\betav}[0]{\ensuremath{\boldsymbol{\beta}}\xspace}
\newcommand{\gammav}[0]{\ensuremath{\boldsymbol{\gamma}}\xspace}
\newcommand{\deltav}[0]{\ensuremath{\boldsymbol{\delta}}\xspace}
\newcommand{\epsilonv}[0]{\ensuremath{\boldsymbol{\epsilon}}\xspace}
\newcommand{\zetav}[0]{\ensuremath{\boldsymbol{\zeta}}\xspace}
\newcommand{\etav}[0]{\ensuremath{\boldsymbol{\eta}}\xspace}
\newcommand{\thetav}[0]{\ensuremath{\boldsymbol{\theta}}\xspace}
\newcommand{\iotav}[0]{\ensuremath{\boldsymbol{\iota}}\xspace}
\newcommand{\kappav}[0]{\ensuremath{\boldsymbol{\kappa}}\xspace}
\newcommand{\lambdav}[0]{\ensuremath{\boldsymbol{\lambda}}\xspace}
\newcommand{\muv}[0]{\ensuremath{\boldsymbol{\mu}}\xspace}
\newcommand{\nuv}[0]{\ensuremath{\boldsymbol{\nu}}\xspace}
\newcommand{\xiv}[0]{\ensuremath{\boldsymbol{\xi}}\xspace}
\newcommand{\omicronv}[0]{\ensuremath{\boldsymbol{\omicron}}\xspace}
\newcommand{\piv}[0]{\ensuremath{\boldsymbol{\pi}}\xspace}
\newcommand{\rhov}[0]{\ensuremath{\boldsymbol{\rho}}\xspace}
\newcommand{\sigmav}[0]{\ensuremath{\boldsymbol{\sigma}}\xspace}
\newcommand{\tauv}[0]{\ensuremath{\boldsymbol{\tau}}\xspace}
\newcommand{\upsilonv}[0]{\ensuremath{\boldsymbol{\upsilon}}\xspace}
\newcommand{\phiv}[0]{\ensuremath{\boldsymbol{\phi}}\xspace}
\newcommand{\chiv}[0]{\ensuremath{\boldsymbol{\chi}}\xspace}
\newcommand{\psiv}[0]{\ensuremath{\boldsymbol{\psi}}\xspace}
\newcommand{\omegav}[0]{\ensuremath{\boldsymbol{\omega}}\xspace}

\newcommand{\varepsilonv}[0]{\ensuremath{\boldsymbol{\varepsilon}}\xspace}
\newcommand{\varthetav}[0]{\ensuremath{\boldsymbol{\vartheta}}\xspace}
\newcommand{\varpiv}[0]{\ensuremath{\boldsymbol{\varpi}}\xspace}
\newcommand{\varrhov}[0]{\ensuremath{\boldsymbol{\varrho}}\xspace}
\newcommand{\varsigmav}[0]{\ensuremath{\boldsymbol{\varsigma}}\xspace}
\newcommand{\varphiv}[0]{\ensuremath{\boldsymbol{\varphi}}\xspace}

\newcommand{\eps}[0]{\ensuremath{\epsilon}\xspace}
\newcommand{\xtilde}[0]{\ensuremath{\widetilde{{\bf x}}}\xspace}
\newcommand{\xhat}[0]{\ensuremath{\widehat{\xv}}\xspace}
\newcommand{\Gauss}[3]{\mathcal{N}\left(#1|#2,#3\right)}

\newcommand{\ang}[1]{\langle{#1}\rangle}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Probabilistic Methods for Divide-and-Conquer Distributed Learning}

\begin{document}

\twocolumn[
\icmltitle{Probabilistic Methods for Divide-and-Conquer Distributed Learning}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2014
% package.
\icmlauthor{Your Name}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute,
            314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute,
            27182 Exp St., Toronto, ON M6H 2T1 CANADA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Distributed Learning, Probabilistic Modelling, Big Data}

\vskip 0.3in
]

\begin{abstract} 

\end{abstract} 

\section{Introduction}

We are now living in the Big Data era, where the large amount of data provide us an opportunity to explain complicated phenomenon or predicting unseen events by machine learning. As many learning problems are formulated on a form of regularized empirical risk minimizations, one important challenge is to design efficient and scalable algorithms to solve them in a large-scale setting. When the size of the data becomes too large for a single machine to store all of the data, or at least to keep the data in memory, one popular solution is to store and process the data in a distributed manner. Consequently, the focus of this work is to study distributed learning algorithms \cite{Bertsekas1989} for empirical risk minimization problems.

Divide-and-conquer approach is one intuitively appealing and simple way of designing distributed learning algorithms, where the data is assumed to be partitioned into blocks across a cluster of computing nodes, and each computing node obtain a local estimate of global parameter based its own data block, and then communicates with other node to reach a  consensus estimate for the global parameter. Divide-and-conquer approach has been studied on a number of problems under the distributed setting, including bootstrap \cite{Kleiner2012}, matrix factorization \cite{Mackey2011}, logistic regression \cite{Mann2009}, kernel ridge regression \cite{YZhang2013} and general smooth convex optimization problems \cite{YZhang2012}. The optimality of the estimate obtained from the divide-and-conquer approach crucially depends on the quality of the local estimators, which in turn depends on the information contained in each block, \eg the size of data block \cite{YZhang2012, YZhang2013}.  However, almost by definition, complicated models that require large-scale data to learn may have at least subsets of local parameters that are poorly estimated using only one block of the data, and therefore it is of interest to infer the confidence of each block-dependent estimate. For a fixed-size dataset, as the number of blocks increases this issue may be exacerbated, as the amount of data on a given node decreases, and hence block-dependent parameter uncertainty may rise. Besides, block-specific bias may exists, this may be caused by the random partitioning of data into blocks, which is called the ``small-sample'' bias problem as studied in \cite{YZhang2012, Scott2013}. 

As our main contribution in this work, to address the aforementioned challenges we propose a series of probabilistic methods that model the uncertainty caused by the random partition (Section \ref{MEM}), facilitate the sharing of statistical strength among data blocks (Section \ref{HM}), and integrate the small-sample bias as part of the model (Section \ref{pADMM}). And we connect the proposed probabilistic methods to some of the well-studied distributed learning algorithms at the end of each section. We also design a generalized Expectation-Maximization (gEM) algorithm for the proposed probabilistic methods, based on which we show that the probabilistic methods inherit similar computational merits of their well-studied non-probabilistic counterparts in the large-scale setting.


\section{Preliminaries}
\subsection{Problem formulation}\label{problem_formulation}

In the following we consider a general regularized empirical risk minimization problem of the form $\min_\theta ~ \sum_{n\in\Omega}l(y_n; \theta)+ h(\theta)$, where $\theta$ is the parameter of interest, which could be a vector or a matrix; $l(y_n; \theta)$ denotes the loss function, which could be non-convex; $h(\theta)$ denotes the regularization function, which could be non-smooth; and $\mathcal{Y} = \{y_n, n\in\Omega\}$ denotes the observed dataset with $y_n$ representing the $n^{th}$ instance and $\Omega$ representing the dataset's index set. In a large-scale setting, the size of the dataset $|\mathcal{Y}|$ becomes too large for a single machine to store all of the data, or at least to keep the data in memory. In this work we focus on the case for which $\mathcal{Y}$ is stored in a distributed manner by a cluster of computing nodes connected through a network. More concretely, we assume $\mathcal{Y}$ is (randomly) partitioned into $B$ blocks, where $B$ may equal to the number of computing nodes in a cluster. We denote each block as $\mathcal{Y}_b = \{y_n, n \in \Omega_b\}$, with $\Omega_b$ representing the $b^{th}$ data block's index set. In a distributed setting, the regularized empirical risk minimization problem is formulated as
\begin{equation}\label{general_obj}
\textstyle\min_{\theta} f(\theta) = \sum_{b=1}^Bl(\mathcal{Y}_b; \theta)+h(\theta)
\end{equation}
where $l(\mathcal{Y}_b; \theta) = \sum_{n\in\Omega_b}l(y_n; \theta)$ is the loss function for data block $b$. In the following we focus on distributed learning algorithms solving (\ref{general_obj}), and $\hat\theta = \argmin_\theta f(\theta)$ will be used to represent a (typically) local-optimal solution.

\subsection{Average mixture based algorithms}\label{AVGM}

One of the well studied distributed learning algorithms for (\ref{general_obj}) is the average mixture (AVGM) method \cite{Mann2009, Zinkevich2010, YZhang2012}. The basic idea of AVGM is to divide the dataset into blocks as stated in Section \ref{problem_formulation}, and each block $b$ is augmented with a local parameter $\theta_b$, and the estimate of $\theta_b$, denoted as $\hat\theta_b$, is computed based on $\mathcal{Y}_b$. Finally the estimate of global parameter $\theta$ is obtained by taking a (weighted) average over the local estimates $\hat\theta_b$. AVGM is summarized as
\begin{equation}\label{avgm}
\hat\theta_b = \textstyle\argmin_{\theta_b}f(\theta_b), \quad \hat\theta = \textstyle\sum_{b=1}^B w_b\hat\theta_b/\sum_{b=1}^Bw_b
\end{equation} 
here $f(\theta_b)$ is the local objective function
\begin{equation}\label{avgm_local}
\textstyle f(\theta_b) = l(\mathcal{Y}_b; \theta_b)+\frac{1}{B}h(\theta_b)
\end{equation}
and $w_b$ is some weight parameter commonly set to $1/B$ \cite{Mann2009, Zinkevich2010} or proportional to $|\mathcal{Y}_n|$ in \cite{YZhang2012}. 


%Prof. Carin, in the literature the "small-sample bias" means the actual bias introduced by partitioning the data into blocks, for example, the overall mean of the data is 0, but for each data block, the block-specific mean may not be 0 anymore. As a result, the local estimators might be biased. So I wish to emphasize the "uncertainty" and "bias" separately here. Also
AVGM is categorized as one type of ``divide-and-conquer'' method, where the dataset is divided into blocks with a local estimate computed for each data block; the global estimate is obtained from a consensus of the local estimates. As theoretically and empirically demonstrated in \cite{YZhang2012}, statistical optimality is retained for AVGM given enough information per block, \eg when $|\mathcal{Y}_b|$ is not too small. However, almost by definition, complicated models that require large-scale data to learn may have at least subsets of local parameters $\theta_b$ that are poorly estimated based only on $\mathcal{Y}_b$, and therefore it is of interest to infer the confidence of each block-dependent estimate. For a fixed-size dataset, as the number of blocks $B$ increases this issue may be exacerbated, as the amount of data on a given node decreases, and hence block-dependent parameter uncertainty may rise. Besides, block-specific bias may exists, this may be caused by the random partitioning of data into blocks, which is called the ``small-sample'' bias problem as studied in \cite{YZhang2012, Scott2013}. To address these challenges, in the following sections we focus on developing a series of probabilistic distributed methods as complements to their non-probabilistic counterparts. The proposed models infer the uncertainty of estimates within each block, and account for this when making overall estimates based on data from all $B$ blocks.


\section{Probabilistic Divide-and-Conquer Methods}\label{proposed_frameworks}


In the proposed probabilistic methods, instead of finding a point estimate $\theta_b$ for each block $b$, we model $\theta_b$ as a latent (unobserved) random variable and consider a more general problem of finding a distribution $q(\theta_b)$ over all possible configurations of it. In the following we first discuss modelling frameworks built on this idea, and then discuss the corresponding learning algorithm that enables the probabilistic methods to share similar computational merits of their well-studied non-probabilistic counterparts in the large-scale setting \textbf{[REFS]}.

%Prof. Carin, what type of reference do we want here? We proposed three methods in the following subsections, and we connected each of them to their non-probabilistic counterparts in the end of each subsection, with corresponding references. For example, MEM -> AVGM, HM -> Penalty method, pADMM -> ADMM.

\subsection{Modeling the uncertainty}\label{MEM}

Perhaps the most straightforward way of formulating an empirical-risk-minimization problem with respect to a distribution is via the maximum entropy principle, which is well-studied in the natural language processing literature \cite{Berger1996}. Under the maximum entropy principle, the problem becomes one of finding a distribution $\hat q(\theta_b)$, such that ($i$) we minimize the convex combination of local objective functions (\ref{avgm_local}) of block $b$ with respect to $\hat q(\theta_b)$, denoted as $\mathbb{E}_{\hat q(\theta_b)}[f(\theta_b)] = \int f(\theta_b)\hat q(\theta_b) d\theta_b$; and ($ii$) at the same time we maximize the entropy of $\hat q(\theta_b)$, denoted $\mathbb{H}[\hat q(\theta_b)] = -\int \hat q(\theta_b)\log  \hat q(\theta_b) d\theta$. The problem is formulated as
\begin{equation}\label{mem1}
\textstyle\hat q(\theta_b) =\argmin_{q(\theta_b)} -\mathbb{H}[q(\theta_b)] +  \mathbb{E}_{q(\theta_b) }[f(\theta_b)]
\end{equation}
After obtaining $\hat q(\theta_b)$, $\hat\theta$ is estimated as a (weighted) average of the mean of $\hat q(\theta_b)$, similar to AVGM: 
\begin{equation}\label{mem2}
\hat\theta = \textstyle\sum_{b=1}^Bw_b\mathbb{E}_{\hat q(\theta_b) }[\theta_b]/\sum_{b=1}^Bw_b
\end{equation}
In the following we refer to (\ref{mem1}) and (\ref{mem2}) as the Maximum Entropy Mixture (MEM) method. 

Concerning (\ref{mem1}), by maximizing $\mathbb{H}[q(\theta_b)]$, we seek a rich/diverse set of possible parameters $\theta_b$ with which to estimate the global parameter $\theta$, with the probability of $\theta_b$ represented by $q(\theta_b)$. However, we simultaneously want this rich set of parameters to fit the model well, in that the expected value of $f(\theta_b)$ with respect to $q(\theta_b)$ is small. Note that based on the estimated set of distributions $\{\hat q(\theta_1), \ldots, \hat q(\theta_B)\}$, we can assess the quality of the estimator $\hat\theta$, \eg calculating the credible/confidence interval of $\hat\theta$, similar to the Bootstrap-based methods in \cite{Kleiner2012}. To relate our method to AVGM, here we focus on the case for which a point estimate of $\hat\theta$ is of interest, taken as a (weighted) average of the mean of the parameters $\theta_b$, as in (\ref{mem2}).

After some simple algebra, it is shown that the optimal solution for (\ref{mem1}) has the following simple form
\begin{equation}\label{mem_solution}
\textstyle \hat q(\theta_b) = \frac{1}{Z}e^{-f(\theta_b)}
\end{equation}
where $Z = \int e^{-f(\theta_b)} d\theta_b$ is the normalization constant. Note that AVGM actually finds the maximum likelihood estimate of (\ref{mem_solution}) with respect to $\theta_b$, which suggests that AVGM is a special case of MEM.

%A different way to relate MEM to AVGM: Note that when $\hat q(\theta_b)$ is restricted to be a Dirac delta distribution with point mass at $\tilde\theta_b$, \eg $\hat q(\theta_b) = \delta_{\tilde\theta_b}(\theta_b)$, it's easy to show that $\tilde\theta_b = \hat\theta_b$, where the later corresponds to the empirical local minimizer of AVGM in (\ref{avgm})

\subsection{Sharing the statistical strength}\label{HM}

One common feature of the AVGM and MEM methods is that the local estimators only use data from the their own block, \ie $\theta_b$ only depends on $\mathcal{Y}_b$, and is independent of data from other blocks. This strategy leads to communication-efficient distributed algorithms, because different computing nodes only need to communicate once when estimating the global parameter $\theta$. However, as discussed at the beginning of Section \ref{proposed_frameworks}, the information contained in one single block may not be sufficient to obtain an accurate estimate, and it could be helpful to share the statistical strength across data blocks when estimating the local parameters.

We propose a method that facilitates the sharing of statistical strength among data blocks, through hierarchical modelling and Bayesian integration of prior information. Similar to MEM, we model $\theta_b$ as a latent random variable, with interest in finding the distribution $q(\theta_b)$. Toward that end we build a hierarchical model, placing prior $p(\theta_b | \theta)$ over each latent random variable $\theta_b$, such that $\theta_b$ is dependent on the global parameter $\theta$, but is conditionally independent of all other latent variables variables given $\theta$. To incorporate the prior and hierarchical information into the empirical risk minimization framework, together with the convex combination of local objective functions, the distance between $q(\theta_b)$ and the prior distribution $p(\theta_b|\theta)$ is minimized, with the distance between distributions measured by the Kullback-Leibler (KL) divergence. The proposed hierarchical model (referred to as HM in the following sections) is summarized as follows
\begin{equation}\label{hm}
\textstyle \min_{\theta, q(\cdot)} ~ \sum_{b=1}^B f(q(\theta_b), \theta) + h(\theta)
\end{equation}
where $q(\cdot) = \{q(\theta_1), \ldots, q(\theta_B)\}$ and the local objective function is defined as
\begin{equation}\label{local_obj}
\textstyle f(q(\theta_b), \theta)  = \mathbb{D}[q(\theta_b)||p(\theta_b|\theta)] + \mathbb{E}_{q(\theta_b)}[l(\mathcal{Y}_b; \theta_b)]
\end{equation}
where $\mathbb{D}[q(\theta_b)||p(\theta_b|\theta)] = \int q(\theta_b)\log\frac{q(\theta_b)}{p(\theta_b|\theta)}d\theta_b$ is the KL divergence between $q(\theta_b)$ and $p(\theta_b|\theta)$. Note that MEM is recovered by letting $p(\theta_b|\theta)$ to be an uninformative (possibly improper) prior. 

Through the introduced hierarchy between data block $\mathcal{Y}_b$, $\theta_b$ and $\theta$, the induced conditional independence suggests that $\mathcal{Y}_b$ and $\theta_b$ still is stored independent of other blocks in a distributed fashion, while the dependency between all latent variables $\{\theta_b\}_{b=1\ldots B}$ and global parameter $\theta$ via prior distribution $p(\theta_b|\theta)$ facilitates the sharing of statistical strength across different blocks. The HM formulation naturally leads to a generalized Expectation-Maximization (gEM) algorithm\footnote{Termed a \emph{generalized} EM because some loss functions $l(\mathcal{Y}_b; \theta_b)$ can't be interpreted as a likelihood function, as discussed below.} to estimate $\theta$ and $q(\theta_b)$ within the same framework, instead of two independent stages as in AVGM and MEM.

The gEM algorithm proceeds by iteratively applying two steps: the E-step finds a distribution $q(\theta_b)$ that minimizes objective function (\ref{hm}) while fixing $\theta$, and evaluate the expectation and KL-divergence in (\ref{local_obj}) with respect to $q(\theta_b)$; and the M-step in turn finds an estimator $\theta$ that minimizes (\ref{hm}) while fixing $q(\theta_b)$. More concretely, in the $k^{th}$ iteration, the E-step updates $q^{k+1}(\theta_b)$ by minimizing $f(q(\theta_b), \theta^k)$, which is shown has the following optimal solution: 
\begin{equation}\label{hm_e_step_sol}
\textstyle q^{k+1}(\theta_b) = \frac{1}{Z}p(\theta_b|\theta^k)e^{-l(\mathcal{Y}_b; \theta_b)}
\end{equation}
where $Z = \int_{\theta_b}p(\theta_b|\theta^k)e^{-l(\mathcal{Y}_b; \theta_b)}d\theta_b$ is the normalization constant.  Given $q^{k+1}(\theta_b)$, the M-step consists of updating $\theta^{k+1}$ by solving the following problem
\begin{equation}\label{hm_m_step}
\textstyle\min_{\theta}~\sum_{b=1}^B\mathbb{E}_{q^{k+1}(\theta_b)}[-\log p(\theta_b|\theta)] + h(\theta)
\end{equation}

Note that minimizing (\ref{local_obj}) with respect to $q(\theta_b)$ in the E-step is closely related to Bayes theorem: $p(\theta_b|\mathcal{Y}_b, \theta) = \frac{p(\theta_b|\theta)p(\mathcal{Y}_b|\theta_b)}{\int p(\theta_b|\theta)p(\mathcal{Y}_b|\theta_b)d\theta_b}$, with $p(\theta_b|\theta)$ and $p(\mathcal{Y}_b|\theta_b)$ representing the prior distribution density and likelihood function, respectively. Zellner \yrcite{Zellner1988} shows that the Bayesian posterior density $p(\theta_b|\mathcal{Y}_b, \theta)$ can equivalently be found by solving the following minimization problem:
\begin{equation}\label{bayes_rule}
\textstyle \min_{q(\theta_b)} ~\mathbb{D}[q(\theta_b)||p(\theta_b|\theta)] + \mathbb{E}_{q(\theta_b)}[-\log p(\mathcal{Y}_b|\theta_b)]
\end{equation}
By comparing (\ref{local_obj}) with (\ref{bayes_rule}), we see that if $p(\mathcal{Y}_b|\theta_b) \propto e^{-l(\mathcal{Y}_b; \theta_b)}$, \ie when $l(\mathcal{Y}_b; \theta_b)$ is interpreted as a negtive log-likelihood function, then $q^{k+1}(\theta_b)$ in (\ref{hm_e_step_sol}) is exactly the posterior distribution $p(\theta_b|\mathcal{Y}_b, \theta^k)$, \ie $q^{k+1}(\theta_b) = p(\theta_b|\mathcal{Y}_b, \theta^k)$. In this case, the gEM algorithm reduces to the classic EM algorithm for statistical models. In many scenarios the loss function $l(\mathcal{Y}_b; \theta_b)$ does have a probabilistic interpretation, \eg  squared $\ell_2$ and $\ell_1$ loss functions are proportional to the  Gaussian and Laplace negative log-likelihood function, respectively; however, some important loss functions don't have a probabilistic counterpart, \eg the hinge loss function for max-margin methods.

Related frameworks to (\ref{hm}) and (\ref{local_obj}) are found in the literature for different applications. Some are known as the minimum relative entropy method \cite{Jaakkola1999, Zhu2012} for max-margin based discriminative learning tasks, or the posterior regularization method \cite{Ganchev2010} and constrained Bayesian inference \cite{Koyejo2013} for incorporating constraints into posterior inference.

%need a better section title here
\subsection{Modelling the local bias}\label{pADMM}

\begin{algorithm}[tb]
   \caption{pADMM}
   \label{alg:pADMM}
\begin{algorithmic}
   \STATE {\bfseries Initialize:} $\theta^0, \rho^0, \{q^0(\theta_b), \lambda_b^0, \gamma_b^0\}_{b=1\ldots B}$
   \FOR{$k= 0, 1, 2, \ldots$}
   \STATE E-step: 
   \FOR{$b=1$ {\bfseries to} $B$ in parallel}
   \STATE Update $q^{k+1}(\theta_b), \lambda_b^{k+1}$ based on (\ref{padmm_primal_sol}), (\ref{padmm_dual_sol}).
   \STATE Update $\gamma_b^{k+1}$ based on (\ref{em_bayes}).
   \ENDFOR
   \STATE M-step:
   \STATE Update $\theta^{k+1}$ by solving (\ref{padmm_m_step}) and $\rho^{k+1}$ based on (\ref{em_bayes})
   \ENDFOR
\end{algorithmic}
\end{algorithm}
As discussed at the beginning of Section \ref{proposed_frameworks}, in the large-scale setting, the dataset is (pre-)partitioned into blocks, and some blocks may provide unreliable or biased estimates, particularly as the amount of data in each block diminishes (\eg as one scales to a large number of nodes). In AVGM, this problem is addressed with the bootstrap/jackknife bias-correction method, as a post-processing step, where the local estimates are shifted to avoid an accumulation of biases in the global estimate \cite{YZhang2012, Scott2013}. Similar ideas may be applied to the MEM method.

In this section we explicitly model the local bias as integrated within the HM method. The basic idea is to constrain the latent random variables $\theta_b$ to be consistent with each other as well as the global parameter $\theta$ in expectation, which is equivalent to constraining $q(\theta_b)$ to share the same mean across all blocks. Note that although the means of $q(\theta_b)$ are now fixed to be the same, these distributions may still be flexible enough to allow block-dependent variability. The optimization problem becomes
\begin{equation}\label{padmm}
\begin{gathered}
\textstyle\min_{\theta, q(\cdot)} \quad\textstyle\sum_{b=1}^B f(q(\theta_b), \theta) + h(\theta)\\
\textstyle\st \quad \mathbb{E}_{q(\theta_b)}[\theta_b] = \theta, ~ b = 1, \ldots, B
\end{gathered}
\end{equation}
where $f(q(\theta_b), \theta)$ is the local objective function defined in (\ref{local_obj}). We name this method pADMM, for probabilistic Alternating Direction Method of Multipliers; we will show below that pADMM is closely related to Alternating Direction Method of Multipliers (ADMM). In order to make their connection transparent, in the following we focus on a special case when $p(\theta_b|\theta)$ is Gaussian:
\begin{equation}\label{gaussian_prior}
\textstyle p(\theta_b|\theta) = \mathcal{N}(\theta_b | \theta, (\rho\gamma_b)^{-1}\Imat)
\end{equation}
with mean $\theta$ and isotropic covariance matrix $(\rho\gamma_b)^{-1}\Imat$, where $\Imat$ denotes the identity matrix. In the following $\rho$ and $\gamma_b$ will be referred to as the global and local precision parameter, respectively. Later in this section we will show $\rho$ and $\gamma_b$ are learned from data, which empirically will lead to faster convergence of the algorithm, and thus less communication between computing nodes.

In the next we apply the gEM algorithm discussed in Section \ref{HM} to pADMM. With the Gaussian assumption in (\ref{gaussian_prior}), the local objective function $f(q(\theta_b), \theta)$ becomes (with constants independent of $q(\theta_b)$ and $\theta$ ignored):
\begin{equation}\label{local_objective_gaussian}
f(q(\theta_b), \theta) = \mathbb{E}_{q(\theta_b)}\left[\frac{\rho\gamma_b}{2}||\theta_b - \theta||_2^2 + l(\mathcal{Y}_b; \theta_b)\right]
\end{equation}
We have the following Lagrangian for each local optimization problem
\begin{equation}\label{padmm_lagrangian}
\begin{gathered}
\textstyle \min_{q(\theta_b), \lambda_b} ~ f(q(\theta_b), \theta) + \lambda_b\cdot(\mathbb{E}_{q(\theta_b)}[\theta_b]- \theta)
\end{gathered}
\end{equation}
where $f(q(\theta_b), \theta)$ is defined in (\ref{local_objective_gaussian}), $\lambda_b$ is the Lagrange dual variable, and $\cdot$ represents the dot product. The dual of (\ref{padmm_lagrangian}) is readily derived, and in the E-step we propose to solve problem (\ref{padmm_lagrangian}) via dual ascent, \eg the dual problem is solved using gradient ascent. With fixed dual variable and global parameter, the updating equation of $q(\theta_b)$ is:
\begin{equation}\label{padmm_primal_sol}
\textstyle q^{k+1}(\theta_b) = \frac{1}{Z}e^{-l(\mathcal{Y}_b; \theta_b) -  \frac{\rho\gamma_b}{2}||\theta_b-\theta^k + \mu^k_b||^2_2}
\end{equation}
where $Z = \int e^{-l(\mathcal{Y}_b; \theta_b) -  \frac{\rho\gamma_b}{2}||\theta_b-\theta^k + \mu^k_b||^2_2} d\theta_b$ is the normalization constant. $\mu^k_b = \frac{1}{\rho\gamma_b}\lambda^k_b$ is the scaled dual variable, which acts in a similar role to the Bootstrap adjustment in \cite{YZhang2012}, and pADMM reduces to HM by fixing $\lambda^k_b = 0$. Inspired by ADMM (discussed in Section \ref{ADMM}), the dual variable $\lambda_b$ is updated using gradient ascent with step size $\rho\gamma_b$:
\begin{equation}\label{padmm_dual_sol}
\lambda^{k+1}_b = \lambda^k_b + \rho\gamma_b(\mathbb{E}_{q^{k+1}(\theta_b)}[\theta_b] - \theta^k)
\end{equation}
Finally, given $q^{k+1}(\theta_b)$ and $\lambda^{k+1}_b$ obtained from the E-step for each local block $b$, the M-step updates $\theta^{k+1}$ by solving the following minimization problem:
\begin{equation}\label{padmm_m_step}
\textstyle \min_{\theta} ~\sum_{b=1}^B\frac{\rho\gamma_b}{2}||\theta-\mathbb{E}_{q^{k+1}(\theta_b)}[\theta_b] + \mu^{k+1}_b||^2_2 + h(\theta)
\end{equation}
where $\mathbb{E}_{q^{k+1}(\theta_b)}[\theta_b]$ is simply the mean of $q^{k+1}(\theta_b)$, and recall that $\mu^{k+1}_b$ is the scaled dual variable. An optimization problem of form (\ref{padmm_m_step}) generally solved efficiently by proximal gradient methods \cite{Parikh13}. As an example, when $h(\theta) = \beta(\alpha||\theta||_1 + (1-\alpha)||\theta||^2_2)$ corresponding to a convex combination of the $\ell_1$ and $\ell_2$ regularizer with $0\le\alpha\le 1$ and $\beta\ge0$, the update for $\theta^{k+1}$ is
\begin{equation}\label{elastic_net}
 \theta^{k+1} = \frac{S(\textstyle\sum_{b=1}^B\rho\gamma_b\mathbb{E}_{q^{k+1}}[\theta_b], \beta\alpha )}{\sum_{b=1}^B\rho\gamma_b + \beta(1-\alpha)}
\end{equation}
where $S(a, b) = \mbox{sign}(a)(|a|-b)_{+}$ is the soft thresholding operator \cite{Friedman2010}. Note that (\ref{elastic_net}) resembles the weighted average strategy in (\ref{avgm}), however, instead of fixing $w_b$ to $1/B$ or proportional to $|\Omega_b|$, the weight corresponds to the product of global and local precision parameters $\rho\gamma_b$. In the next we propose to update $\rho$ and $\gamma_b$ in the M-step, done by minimizing (\ref{padmm}) with respect to $\rho$ and $\gamma_b$ in turn. Denoting the expected squared residual as $r^{k+1} = \mathbb{E}_{q^{k+1}(\theta_b)}[||\theta_b^{k+1}-\theta^{k+1}+\mu_b^{k+1}||_2^2]$, the update equations for $\rho^{k+1}$ and $\gamma_b^{k+1}$ is
\begin{equation}\label{em_bayes}
\begin{gathered}
\textstyle \rho^{k+1} = \frac{BP}{\sum_{b=1}^B\gamma_b^kr^{k+1}}, \quad \gamma_b^{k+1} = \frac{P}{\rho^{k+1}r^{k+1}}
\end{gathered}
\end{equation}
where $P$ denotes the dimension of $\theta_b$. The pADMM method is summarized in Algorithm \ref{alg:pADMM}; note that by setting $\mu_b = 0$, pADMM reduces to HM. Note that, under the Gaussian assumption (\ref{gaussian_prior}) for the HM method with $\gamma_b^k$ fixed and with $\rho^k$ set to be an increasing sequence, \eg $\rho^1 < \rho^2 < \ldots$ and $\lim_{k\to\infty}\rho^{k} = \infty$, it reduces to the penalty methods \cite{Bertsekas1996} when only a point esimate of $\theta_b$ is of interest.

\subsection{Full Bayesian treatment}\label{Full_Bayesian}
Although the above proposed methods, \ie MEM, HM and pADMM, are motivated by the optimization problem (\ref{general_obj}), they can naturally be extended to a fully Bayesian setting. As shown by (\ref{bayes_rule}), the gEM algorithms proposed for HM and pADMM reduce to classic EM algorithms when $l(\mathcal{Y}_b; \theta_b)$ corresponds to a negative log-likelihood function. Based on this observation, if the global parameter $\theta$ is also modeled as a random variable, and the regularization function $h(\theta)$ in (\ref{general_obj}) is replaced by a prior distribution, posterior inference is performed on $\theta$, and then we have a fully Bayesian model. However, this involves the design of an efficient inference algorithm for Bayesian posterior computation, and is beyond the scope of this work. \cite{Scott2013, Neiswanger2013} represent two recent works in this direction, although their frameworks are both based on the AVGM setting.


\section{Practical Issues of the gEM Algorithm}\label{variational_EM}
One important issue in making the gEM practical for large-scale data is to evaluate the expectations efficiently in the E-step, \eg $\mathbb{E}_{q(\theta_b)}[\theta_b]$, which in turn generally requires an analytical expression of the distribution $q(\theta_b)$, or at least the ability to draw samples from it. Given the expectations found in the E-step, efficient optimization algorithms, \eg the proximal gradient method or stochastic gradient based methods, becomes applicable for the M-step. Consequently, the first key technical component in our implementation is adoption of variational methods \cite{Wainwright2008} in the E-step, which transforms the gEM algorithm to a variational EM algorithm; if we extend our method to be full Bayesian as discussed in Section \ref{Full_Bayesian}, gEM becomes the variational Bayesian inference algorithm \cite{Bishop2006}.

As discussed in Sections \ref{MEM}-\ref{pADMM}, for MEM, HM and pADMM the distribution $q(\theta_b)$ of interest is found by minimizing the local objective function (\ref{local_obj}), which could be problematic when the expectation $\mathbb{E}_{q(\theta_b)}[l(\mathcal{Y}_b; \theta_b)]$ is not analytic. Two types of variational methods are applied here, commonly referred to as local and global variational methods in the literature \cite{Bishop2006}. Local variational methods first find an upper bound of the loss function $\tilde l(\mathcal{Y}_b; \theta_b; \xi_b) \ge  l(\mathcal{Y}_b; \theta_b)$, then the expectation of the upper bound $\mathbb{E}_{q(\theta_b)}[\tilde l(\mathcal{Y}_b; \theta_b; \xi_b)]$ is minimized to find $q(\theta_b)$, with $\xi_b$ representing some variational parameters that is optimized to tighten the gap between $\tilde l(\mathcal{Y}_b; \theta_b; \xi_b)$ and $l(\mathcal{Y}_b; \theta_b)$. Local variational method are well studied for various types of loss functions and likelihood functions, a few examples include \cite{Zhu2012} for hinge loss function, \cite{Jaakkola2000, Khan2010} for the logistic/soft-max loss function, and \cite{Khan2013, Wang2013} for general non-conjugate likelihood functions. 

Global variational methods, on the other hand, directly seeks an approximation to the true underlying distribution over all random variables by restricting the range of $q(\theta)$ over which the optimization is performed, \eg the mean field methods restrict $q(\theta)$ to take factorized forms. Mean field based variational methods are found in many Bayesian hierarchical models with latent variables and intractable posterior distributions \cite{Wainwright2008}. Besides their common usage, here we also propose to use mean field variational methods from a computational perspective, in that by restricting $q(\theta) = \prod_i q(\theta_i)$ to be fully factorized across its components, the variational methods proceeds by updating each scalar based density $q(\theta_i)$ in turn. Although such fully factorized constraint may lead to less accurate estimate of $q(\theta)$, it avoids computing high-dimensional quantities, \eg the covariance matrix of $\theta$, and it is efficient and scalable in large-scale applications, which resembles some recent success of the coordinate descent algorithms \cite{Friedman2010, Bradley2011, Yuan2012, Yu2013}.

Although sampling based methods, \eg MCMC and importance re-sampling, generally doen't need to evaluate the normalization constant $Z$ for $q(\theta_b)$ explicitly, and can readily be applied to our framework, the reason we prefer variational method to sampling is two-fold: ($i$) It is generally more difficult to monitor the convergence of a sampling based method than deterministic variational method; and ($ii$) our preliminary experiment suggests that in the high-dimensional and large-scale setting MCMC tends to take more iterations to reach a reasonable solution than variational method, and importance re-sampling tends to collapse to a single point.


\section{Related Distributed Learning Methods}

\subsection{Distributed learning via ADMM}\label{ADMM}
ADMM has generated much attention for a variety of applications recently; a comprehensive survey of ADMM, especially its application to distributed learning, is found in \cite{Boyd10}. Here we only summarize important aspects of ADMM. First note that we can rewrite (\ref{general_obj}) into the following equivalent problem:
\begin{equation}\label{admm_obj}
\begin{gathered}
\min_{\theta, \theta_b} ~ \textstyle\sum_{b=1}^Bl(\mathcal{Y}_b,\theta_b) + h(\theta)\\
\st ~ \theta_b - \theta = 0, ~ b = 1,\ldots,B
\end{gathered}
\end{equation}
The ADMM formulation for the problem (\ref{admm_obj}) is derived directly from the following augmented Lagrangian
\begin{equation}\label{admm_lag_global}
\textstyle L_\rho(\{\theta_b, \lambda_b\}_{b=1}^B, \theta) =  \sum_{b=1}^BL_\rho(\theta_b, \lambda_b, \theta) + h(\theta)
\end{equation}
Here $L_\rho(\theta_b, \lambda_b, \theta)$ is the local augmented Lagrangian:
\begin{equation}\label{admm_lag_local}
\textstyle L_\rho(\theta_b, \lambda_b, \theta)  = l(\mathcal{Y}_b, \theta_b) + \lambda_b\cdot(\theta_b - \theta) + \frac{\rho}{2}||\theta_b - \theta||_2^2
\end{equation}
where $\rho \ge 0$ is the tuning parameter of the augmented Lagrangian. ADMM proceeds by iteratively updating $\theta_b, \lambda_b$ for each block $b$ independently and $\theta$:
\begin{equation}\label{admm_update_local}
\textstyle \theta_b^{k+1} = \argmin_{\theta_b}~L_\gamma(\theta_b, \lambda_b^k, \theta^k)
\end{equation}
\begin{equation}\label{admm_update_lag}
\textstyle \lambda_b^{k+1} = \lambda_b^k + \rho(\theta_b^{k+1} - \theta^{k})
\end{equation}
\begin{equation}\label{admm_update_global}
\theta^{k+1} = \argmin_\theta ~\textstyle\sum_{b=1}^B\frac{\rho}{2}||\theta_b^{k+1} - \theta + \mu_b^{k+1}||^2_2 + h(\theta)
\end{equation}
Note that in the gradient ascent update in (\ref{admm_update_lag}) the step size is chosen to be $\rho$. The motivation behind this choice is explained by Boyd el al. \yrcite{Boyd10}, that by using $\rho$ as the step size, the iterate
$\theta_b^{k+1}, \lambda_b^{k+1}$ in (\ref{admm_update_local} - \ref{admm_update_lag}) is dual feasible, and as the ADMM proceeds the primal residual $\theta_b -\theta$ converges to zero, together with the dual feasible condition the procedure (\ref{admm_update_local} - \ref{admm_update_global}) will yield optimal solution $\hat\theta$. 

Now comparing the update equations for ADMM in (\ref{admm_update_local} - \ref{admm_update_global}) with that of pADMM (\ref{local_objective_gaussian}, \ref{padmm_primal_sol} - \ref{padmm_m_step}), we see that the gEM algorithm for pADMM may be thought of as a probabilistic version of ADMM, hence the name. On the other hand, ADMM may be thought of as a way of finding a point estimate for pADMM,which again resembles the connections between AVGM and MEM, and HM and penalty methods. Note that carefully tuning the parameter $\rho$ is one caveat of reaching fast convergence of ADMM, which is commonly done by cross-validation, which could be particularly computationally expensive in a large-scale setting, as one need multiple passes over the whole dataset. However, as mentioned above, fast convergence is important for iterative algorithms in the distributed learning, as fewer iterations means less expensive communication among computing nodes; this is particularly critical when the dimension of the parameter is high. While in pADMM, $\rho$ is updated automatically as in (\ref{em_bayes}). Note that in pADMM the block-specific precision parameter $\gamma_b$ is modelled explicitly and also updated in (\ref{em_bayes}), which leads to faster convergence as will be showed empirically through experiments, while for ADMM modelling the block-specific parameters will unfortunately increase the number of parameters to tune.

\subsection{Distributed (sub-)gradient method}
We have been focused on the``divide-and-conquer" type of frameworks thus far, and in this section we briefly discuss another type of distributed learning algorithm, the distributed sub-gradient method, which will serve as an important baseline when we compare different distributed algorithm empirically through experiments. For the distributed sub-gradient method for optimization problems (\ref{general_obj}), in each iteration the sub-gradients $\partial l(\mathcal{Y}_b; \theta)$ are computed independently for each block $b$, and these separate sub-gradients are then summed up to compute the exact global sub-gradients $\textstyle\sum_{b=1}^B\partial l(\mathcal{Y}_b; \theta)$, which are used to perform the optimization step and update the parameter $\theta$ received by all $B$ blocks for the next iteration's sub-gradients computation. The distributed sub-gradient method is guaranteed to find the (local) optimal solution, however, it requires relatively many iterations before convergence, which in turn requires frequent communications among nodes in the cluster. On the other hand, the asymptotically optimal AVGM and MEM methods represent another extreme of the distributed learning framework, where communication only happens once, but the solution might not be optimal when each data block is insufficient.

\section{Empirical Study}
In this section we solve two challenging problems with the proposed methods and the variational gEM algorithm described above: the non-smooth $\ell_1$-regularized logistic regression, and the non-convex low-rank matrix completion. In the following we first discuss the experimental setting.

\subsection{Experimental setting}

The distributed statistical optimization experimental environment is set up by the Spark cluster computing framework \cite{Zaharia2010} of version 0.8.1\footnote{\url{http://spark.incubator.apache.org}}. Spark provides a fault-tolerant abstraction for in-memory fast iterative computing, that can run on either a single multi-core machine or cluster with up to hundreds of computing nodes. With Spark we conduct experiments on both multi-core and distributed environment. In the multi-core setting, we use a 16-core AMD Opteron 6212 processor with 50 Gigabytes memory. For the distributed setting, we build two types of clusters for dataset of different scales. For the moderately large dataset, we build the cluster with Amazon EC2 machines, where each computing node is a general purpose m1.xlarge instance with 4 virtual CPUs and 15 Gigabytes memory \footnote{\url{http://aws.amazon.com/ec2/instance-types}}. For industrially large dataset, the cluster consists of computing nodes each equipped with a 24-core Intel Xeon X5650 processor and 24 Gigabytes memory. All algorithms are implemented with Spark to make fair comparisons.

\subsection{Distributed $\ell_1$-regularized logistic regression}
In logistic regression the each training data $y_n$ contains two parts, $y_n = \{l_n, x_n\}$ where $l_n \in \{-1, +1\}$ represents the label and $x_n$ represents the feature vector. The loss function in (\ref{general_obj}) is $l(\mathbb{Y}_b; \theta) = \sum_{n\in\Omega_b}\log(1+\exp(-l_n(x_n\cdot\theta)))$, and the regularization function is $h(\theta) = C||\theta||_1$, where $C$ is a tuning parameter controls the sparsity of the parameter $\theta$. And we assume the prior distribution $p(\theta_b|\theta)$ is Gaussian as in (\ref{gaussian_prior}) for HM and pADMM. 

In this case $q(\theta_b)$ doesn't readily have an analytical form, as the normalization constant $Z$ in (\ref{mem_solution}) for MEM, (\ref{hm_e_step_sol}) for HM, and (\ref{padmm_primal_sol}) for pADMM are all intractable. As mentioned in Section \ref{variational_EM}, we will use local and global variational methods here. For the local variational method, Laplace variational method (Lap) \cite{Wang2013} and the Bohning bound based variational method (Bohn) \cite{Khan2010} are used to obtain a quadratic approximation of $l(\mathbb{Y}_b; \theta)$, based on which $q(\theta_b)$ becomes tractable. For the global variational (mean-field) method, we assume $q(\theta_b) = \prod_{i=1}^Pq(\theta_{bi})$ where $P$ denotes the number of features. Jaakkola bound based method \cite{Jaakkola2000} are not adopted here because our preliminary results suggest that it doesn't work well with the mean-field assumption. We test the logistic regression on two datasets, one is the  KDDCup2010 (bridge to algebra) dataset, which is commonly used as a benchmark dataset for binary classification, and is public available \footnote{\url{http://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html}}, and we filtered out features that occur less than 5 times across the training examples. The second dataset consists of 60 days of advertising event logs for a major social network site. We use the first 45 days of data as training and the last 15 days data as test. Both the training and test sets consist about 170 and 40 Millions of events. The features are extracted from user and ad campaign information, for instance, the ad campaign features include n-grams, categories, advertiser characteristics, among others. We use mutual information and the minimum support criteria to do feature selection on these features and the two-way interactions between all pairs. Finally we obtain a feature set that contains about 100K binary features. The important statistics for each dataset is summarized in Table \ref{lr_datasets}.

\begin{table}\label{lr_datasets}
\scriptsize
\caption{ Statistics and parameters for each dataset}
\begin{tabular}{c|cccc}
~& $P$ & $|\Omega|$ & $|\bar\Omega|$ & nnz \\
\hline
KDDCup2010 & 4,156,973 & 19,264,097 & 748,401 & 562,968,283\\
Ads Click & 100K & 170M & 40M  & 10B
\end{tabular}
\end{table}


\subsection{Distributed low-rank matrix completion}
In our notation, $\mathcal{Y} = \{y_n, n\in\Omega\}$ corresponds to observed entries of a matrix $\Ymat$ of dimension $I \times J$, $n = (i,j)$ indexes each entry, and $\Omega$ is a subset of $\{1, \ldots, I\}\otimes\{1,\ldots, J\}$ denoting the indexes of the observed entries in $\Ymat$ with $\otimes$ denoting the Cartesian product. And similarly $\theta \in \mathbb{R}^{I\times J}$ denotes the model parameters of interest. The loss function is quadratic $l(\mathcal{Y}; \theta) = \sum_{(i,j)\in\Omega}(y_{i,j} - \theta_{i,j})^2$, and the regularization functions $h(\theta)$ we consider here are the nuclear norm and the $\gamma_2$-norm (also know as the max-norm) \cite{Srebro2004}. We assume $\theta$ has rank at most $K$, in which case $\theta$ can be explicitly written as $UV'$ where $U \in \mathbb{R}^{I\times K}$ and $V \in \mathbb{R}^{J \times K}$. Such approximation transforms the low-rank matrix completion a non-convex problem, with objective function now can be written as
\begin{equation}\label{mf_obj}
\textstyle f(U,V) = \sum_{(i,j)\in\Omega}(y_{i,j} - U_{i\cdot}V_{j\cdot}')^2 + h(U) + h(V)
\end{equation}
Note that the nuclear norm and $\gamma_2$-norm on $\theta$ can be equivalent formulated in terms of $U$ and $V$, such that the nuclear norm can be written as $h(U) = ||U||_F^2 := \sum_{i}\sum_kU_{ik}^2$, and $\gamma_2$-norm can be written as $h(U) = ||U||_{2,\infty}^2 := \max_i(\sum_k U_{ik}^2)$ \cite{Recht2010}. The optimization techniques for both norms that could be used in the M-step of the gEM algorithm can be found at \cite{Salakhutdinov2007, Lee2010}. In the large-scale setting, we randomly partition the observed data $\mathcal{Y}$ into $B$ blocks similar to the two-way random partition method in \cite{Recht2011}, with each block corresponds to a sub-matrix of size approximately $\frac{I}{B_r} \times \frac{J}{B_c}$, such that $B_rB_c = B$, and with parameters $U$ and $V$ conformally partitioned into $B_r$ and $B_c$ blocks respectively. Consequently, the pADMM formulation of the low-rank matrix completion problem is
\begin{equation}\label{mf_padmm}
\begin{gathered}
\textstyle \min_{U,V, q(\cdot)}~ \sum_{b=1}^B f(U_b,V_b, U, V) + h(U) + h(V)\\
\st ~ \mathbb{E}_{q(U_b)} = U_b, ~ \mathbb{E}_{q(V_b)} = V_b \\
\end{gathered}
\end{equation}




\begin{table}\label{mf_alg_comp}
\scriptsize
\caption{Comparison of memory and network usage for two types of matrix factorization algorithms}
\begin{tabular}{c|c|c}
~  &  network per epoch & memory per core\\
\hline
ALS/CD & $\mathcal{O}( (M+N) K B)$  & $\mathcal{O}(MK+NK + \frac{|\Omega|}{B})$  \\
Proposed & $\mathcal{O}( MK B_{c} + NK B_{r})$ & $\mathcal{O}(\frac{MK}{B_{r}}+\frac{NK}{B_{c}} + \frac{|\Omega|}{B})$ \\
\end{tabular}
\end{table}


\begin{table}\label{mf_datasets}
\scriptsize
\caption{ Statistics for each dataset}
\begin{tabular}{c|cccc}
~ & $I$ & $J$ & $\Omega$ & $|\bar\Omega|$ \\
\hline
Netflix & 2,649,429 & 17,770 & 99,072,112 &  1,408,395\\
KDDCup2011 & 1,000,990  & 624,961 &  252,800,275 & 4,003,960\\
Jumbo (Synthetic) & 1,000,000 & 1,000,000 & 1,983,749,510 & 23,328,835
\end{tabular}
\end{table}



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\newpage
\small
\bibliography{icml2014}
\bibliographystyle{icml2014}

\end{document} 
