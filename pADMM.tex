%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2014 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{calc}
\usepackage[T1]{fontenc}
\usepackage{xspace}
\usepackage{amsmath,amssymb}
\usepackage{amsthm,amscd,amsfonts}
\usepackage{bm}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2014}

\newenvironment{packed_enum}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}

\def\x{{\bm x}}
\def\z{{\bm z}}
\def\y{{\bm y}}
\def\s{{\bm s}}
\def\v{{\bm v}}
\def\n{{\bm n}}
\def\L{{\cal L}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\st}{subject~to}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\beqs}{\begin{eqnarray}}
\newcommand{\eeqs}{\end{eqnarray}}
\newcommand{\barr}{\begin{array}}
\newcommand{\earr}{\end{array}}

\newcommand{\Rc}[0]{\ensuremath{\mathcal{R}}\xspace}
\newcommand{\Nc}[0]{\ensuremath{\mathcal{N}}\xspace}
\newcommand{\Gc}[0]{\ensuremath{\mathcal{G}}\xspace}
\newcommand{\Dc}[0]{\ensuremath{\mathcal{D}}\xspace}
\newcommand{\Oc}[0]{\ensuremath{\mathcal{O}}\xspace}
\newcommand{\Wc}[0]{\ensuremath{\mathcal{W}}\xspace}
\newcommand{\E}[0]{\ensuremath{\mathbb{E}}\xspace}

\newcommand{\zerov}[0]{\ensuremath{{\bf 0}}\xspace}
\newcommand{\onev}[0]{\ensuremath{{\bf 1}}\xspace}

\newcommand{\ie}[0]{\emph{i.e., }}
\newcommand{\ea}[0]{\emph{et al. }}
\newcommand{\eg}[0]{\emph{e.g., }}
\newcommand{\cf}[0]{\emph{cf. }}
\newcommand{\etc}[0]{\emph{etc.}}

\newcommand{\Amat}[0]{\ensuremath{{\bf A}}\xspace}
\newcommand{\Bmat}[0]{\ensuremath{{\bf B}}\xspace}
\newcommand{\Cmat}[0]{\ensuremath{{\bf C}}\xspace}
\newcommand{\Dmat}[0]{\ensuremath{{\bf D}}\xspace}
\newcommand{\Emat}[0]{\ensuremath{{\bf E}}\xspace}
\newcommand{\Fmat}[0]{\ensuremath{{\bf F}}\xspace}
\newcommand{\Gmat}[0]{\ensuremath{{\bf G}}\xspace}
\newcommand{\Hmat}[0]{\ensuremath{{\bf H}}\xspace}
\newcommand{\Imat}[0]{\ensuremath{{\bf I}}\xspace}
\newcommand{\Jmat}[0]{\ensuremath{{\bf J}}\xspace}
\newcommand{\Kmat}[0]{\ensuremath{{\bf K}}\xspace}
\newcommand{\Lmat}[0]{\ensuremath{{\bf L}}\xspace}
\newcommand{\Mmat}[0]{\ensuremath{{\bf M}}\xspace}
\newcommand{\Nmat}[0]{\ensuremath{{\bf N}}\xspace}
\newcommand{\Omat}[0]{\ensuremath{{\bf O}}\xspace}
\newcommand{\Pmat}[0]{\ensuremath{{\bf P}}\xspace}
\newcommand{\Qmat}[0]{\ensuremath{{\bf Q}}\xspace}
\newcommand{\Rmat}[0]{\ensuremath{{\bf R}}\xspace}
\newcommand{\Smat}[0]{\ensuremath{{\bf S}}\xspace}
\newcommand{\Tmat}[0]{\ensuremath{{\bf T}}\xspace}
\newcommand{\Umat}[0]{\ensuremath{{\bf U}}\xspace}
\newcommand{\Vmat}[0]{\ensuremath{{\bf V}}\xspace}
\newcommand{\Wmat}[0]{\ensuremath{{\bf W}}\xspace}
\newcommand{\Xmat}[0]{\ensuremath{{\bf X}}\xspace}
\newcommand{\Ymat}[0]{\ensuremath{{\bf Y}}\xspace}
\newcommand{\Zmat}[0]{\ensuremath{{\bf Z}}\xspace}

\newcommand{\1}[0]{\ensuremath{\boldsymbol{1}}\xspace}
\newcommand{\av}[0]{\ensuremath{\boldsymbol{a}}\xspace}
\newcommand{\bv}[0]{\ensuremath{\boldsymbol{b}}\xspace}
\newcommand{\cv}[0]{\ensuremath{\boldsymbol{c}}\xspace}
\newcommand{\dv}[0]{\ensuremath{\boldsymbol{d}}\xspace}
\newcommand{\ev}[0]{\ensuremath{\boldsymbol{e}}\xspace}
\newcommand{\fv}[0]{\ensuremath{\boldsymbol{f}}\xspace}
\newcommand{\gv}[0]{\ensuremath{\boldsymbol{g}}\xspace}
\newcommand{\hv}[0]{\ensuremath{\boldsymbol{h}}\xspace}
\newcommand{\iv}[0]{\ensuremath{\boldsymbol{i}}\xspace}
\newcommand{\jv}[0]{\ensuremath{\boldsymbol{j}}\xspace}
\newcommand{\kv}[0]{\ensuremath{\boldsymbol{k}}\xspace}
\newcommand{\lv}[0]{\ensuremath{\boldsymbol{l}}\xspace}
\newcommand{\mv}[0]{\ensuremath{\boldsymbol{m}}\xspace}
\newcommand{\nv}[0]{\ensuremath{\boldsymbol{n}}\xspace}
\newcommand{\ov}[0]{\ensuremath{\boldsymbol{o}}\xspace}
\newcommand{\pv}[0]{\ensuremath{\boldsymbol{p}}\xspace}
\newcommand{\qv}[0]{\ensuremath{\boldsymbol{q}}\xspace}
\newcommand{\rv}[0]{\ensuremath{\boldsymbol{r}}\xspace}
\newcommand{\sv}[0]{\ensuremath{\boldsymbol{s}}\xspace}
\newcommand{\tv}[0]{\ensuremath{\boldsymbol{t}}\xspace}
\newcommand{\uv}[0]{\ensuremath{\boldsymbol{u}}\xspace}
\newcommand{\vv}[0]{\ensuremath{\boldsymbol{v}}\xspace}
\newcommand{\wv}[0]{\ensuremath{\boldsymbol{w}}\xspace}
\newcommand{\xv}[0]{\ensuremath{\boldsymbol{x}}\xspace}
\newcommand{\yv}[0]{\ensuremath{\boldsymbol{y}}\xspace}
\newcommand{\zv}[0]{\ensuremath{\boldsymbol{z}}\xspace}

\newcommand{\Gammamat}[0]{\ensuremath{\boldsymbol{\Gamma}}\xspace}
\newcommand{\Deltamat}[0]{\ensuremath{\boldsymbol{\Delta}}\xspace}
\newcommand{\Thetamat}[0]{\ensuremath{\boldsymbol{\Theta}}\xspace}
\newcommand{\Lambdamat}[0]{\ensuremath{\boldsymbol{\Lambda}}\xspace}
\newcommand{\Ximat}[0]{\ensuremath{\boldsymbol{\Xi}}\xspace}
\newcommand{\Pimat}[0]{\ensuremath{\boldsymbol{\Pi}}\xspace}
\newcommand{\Sigmamat}[0]{\ensuremath{\boldsymbol{\Sigma}}\xspace}
\newcommand{\Upsilonmat}[0]{\ensuremath{\boldsymbol{\Upsilon}}\xspace}
\newcommand{\Phimat}[0]{\ensuremath{\boldsymbol{\Phi}}\xspace}
\newcommand{\Psimat}[0]{\ensuremath{\boldsymbol{\Psi}}\xspace}
\newcommand{\Omegamat}[0]{\ensuremath{\boldsymbol{\Omega}}\xspace}

\newcommand{\alphav}[0]{\ensuremath{\boldsymbol{\alpha}}\xspace}
\newcommand{\betav}[0]{\ensuremath{\boldsymbol{\beta}}\xspace}
\newcommand{\gammav}[0]{\ensuremath{\boldsymbol{\gamma}}\xspace}
\newcommand{\deltav}[0]{\ensuremath{\boldsymbol{\delta}}\xspace}
\newcommand{\epsilonv}[0]{\ensuremath{\boldsymbol{\epsilon}}\xspace}
\newcommand{\zetav}[0]{\ensuremath{\boldsymbol{\zeta}}\xspace}
\newcommand{\etav}[0]{\ensuremath{\boldsymbol{\eta}}\xspace}
\newcommand{\thetav}[0]{\ensuremath{\boldsymbol{\theta}}\xspace}
\newcommand{\iotav}[0]{\ensuremath{\boldsymbol{\iota}}\xspace}
\newcommand{\kappav}[0]{\ensuremath{\boldsymbol{\kappa}}\xspace}
\newcommand{\lambdav}[0]{\ensuremath{\boldsymbol{\lambda}}\xspace}
\newcommand{\muv}[0]{\ensuremath{\boldsymbol{\mu}}\xspace}
\newcommand{\nuv}[0]{\ensuremath{\boldsymbol{\nu}}\xspace}
\newcommand{\xiv}[0]{\ensuremath{\boldsymbol{\xi}}\xspace}
\newcommand{\omicronv}[0]{\ensuremath{\boldsymbol{\omicron}}\xspace}
\newcommand{\piv}[0]{\ensuremath{\boldsymbol{\pi}}\xspace}
\newcommand{\rhov}[0]{\ensuremath{\boldsymbol{\rho}}\xspace}
\newcommand{\sigmav}[0]{\ensuremath{\boldsymbol{\sigma}}\xspace}
\newcommand{\tauv}[0]{\ensuremath{\boldsymbol{\tau}}\xspace}
\newcommand{\upsilonv}[0]{\ensuremath{\boldsymbol{\upsilon}}\xspace}
\newcommand{\phiv}[0]{\ensuremath{\boldsymbol{\phi}}\xspace}
\newcommand{\chiv}[0]{\ensuremath{\boldsymbol{\chi}}\xspace}
\newcommand{\psiv}[0]{\ensuremath{\boldsymbol{\psi}}\xspace}
\newcommand{\omegav}[0]{\ensuremath{\boldsymbol{\omega}}\xspace}

\newcommand{\varepsilonv}[0]{\ensuremath{\boldsymbol{\varepsilon}}\xspace}
\newcommand{\varthetav}[0]{\ensuremath{\boldsymbol{\vartheta}}\xspace}
\newcommand{\varpiv}[0]{\ensuremath{\boldsymbol{\varpi}}\xspace}
\newcommand{\varrhov}[0]{\ensuremath{\boldsymbol{\varrho}}\xspace}
\newcommand{\varsigmav}[0]{\ensuremath{\boldsymbol{\varsigma}}\xspace}
\newcommand{\varphiv}[0]{\ensuremath{\boldsymbol{\varphi}}\xspace}

\newcommand{\eps}[0]{\ensuremath{\epsilon}\xspace}
\newcommand{\xtilde}[0]{\ensuremath{\widetilde{{\bf x}}}\xspace}
\newcommand{\xhat}[0]{\ensuremath{\widehat{\xv}}\xspace}
\newcommand{\Gauss}[3]{\mathcal{N}\left(#1|#2,#3\right)}

\newcommand{\ang}[1]{\langle{#1}\rangle}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Probabilistic Divide-and-Conquer Methods for Statistical Optimization}

\begin{document}

\twocolumn[
\icmltitle{Probabilistic Divide-and-Conquer Methods for Statistical Optimization}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2014
% package.
\icmlauthor{Your Name}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute,
            314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute,
            27182 Exp St., Toronto, ON M6H 2T1 CANADA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Distributed Learning, Bayesian Inference, Big Data}

\vskip 0.3in
]

\begin{abstract} 

\end{abstract} 

\section{Introduction}

And in sum, the main contributions of this work are ($i$) the distributed statistical optimization problem are formulated into a series of probabilistic divide-and-conquer based methods, which aim to handle the uncertainty (Section \ref{MEM}) and bias (Section \ref{pADMM}) introduced by partitioning data into blocks, while facilitating the sharing of statistical strength among data blocks without increasing too much communication burden (Section \ref{HM}); and ($ii$) a generalized Expectation-Maximization (gEM) algorithm is proposed (Section \ref{proposed_frameworks}), based on which we show that the probabilistic methods inherit similar computational merits of their well-studied non-probabilistic counterparts in the large-scale setting.

\section{Preliminaries}
\subsection{Problem formulation}\label{problem_formulation}
In the following we consider a general regularized empirical risk minimization problem of the form $\min_\theta ~ \sum_{n\in\Omega}l(y_n; \theta)+ h(\theta)$, here $\theta$ is the parameter of interest, which could be a vector or a matrix; $l(y_n; \theta)$ denotes the loss function, which could be non-convex; $h(\theta)$ denotes the regularization function, which could be non-smooth; and $\mathcal{Y} = \{y_n, n\in\Omega\}$ denotes the observed dataset with $y_n$ representing the $n^{th}$ instance and $\Omega$ representing the dataset's index set. In a large-scale setting, the size of the dataset $|\mathcal{Y}|$ becomes too large for a single machine to store all of the data, or at least to keep the data in memory. In this work we focus on the case when $\mathcal{Y}$ is stored distributedly by a cluster of computing nodes connected through network. More concretely, we assume $\mathcal{Y}$ is (randomly) partitioned into $B$ blocks, where $B$ may equal to the number of computing nodes in a cluster, and we denote each block as $\mathcal{Y}_b = \{y_n, n \in \Omega_b\}$ with $\Omega_b$ representing the $b^{th}$ data block's index set. In the distributed setting, the regularized empirical risk minimization problem can be formulated as
\begin{equation}\label{general_obj}
\textstyle\min_{\theta} f(\theta) = \sum_{b=1}^Bl(\mathcal{Y}_b; \theta)+h(\theta)
\end{equation}
where $l(\mathcal{Y}_b; \theta) = \sum_{n\in\Omega_b}l(y_n; \theta)$ is the loss function for data block $b$. In the following we will focus on distributed learning algorithms solving (\ref{general_obj}), and $\hat\theta = \argmin_\theta f(\theta)$ will be used to represent the (local) optimal solution.

\subsection{Average mixture based algorithms}\label{avgm}

One of the well studied distributed learning algorithms for (\ref{general_obj}) is the average mixture (AVGM) method \cite{Mann2009, Zinkevich2010, YZhang2012}. The basic idea of AVGM is to divide the dataset into blocks as stated in Section \ref{problem_formulation}, and each block $b$ is augmented with a local parameter $\theta_b$, and the estimate of $\theta_b$, denoted as $\hat\theta_b$, is computed based on $\mathcal{Y}_b$. Finally the estimate of global parameter $\theta$ is obtained by taking a (weighted) average over the local estimates $\hat\theta_b$. AVGM can be summarized as
\begin{equation}\label{avgm}
\hat\theta_b = \textstyle\argmin_{\theta_b}f(\theta_b), \quad \hat\theta = \textstyle\sum_{b=1}^B w_b\hat\theta_b/\sum_{b=1}^Bw_b
\end{equation} 
here $f(\theta_b)$ is the local objective function
\begin{equation}\label{avgm_local}
\textstyle f(\theta_b) = l(\mathcal{Y}_b; \theta_b)+\frac{1}{B}h(\theta_b)
\end{equation}
and $w_b$ is some weight parameter commonly set to $1/B$ \cite{Mann2009, Zinkevich2010} or proportional to $|\mathcal{Y}_n|$ in \cite{YZhang2012}. 

AVGM can be categorized as one type of the ``divide-and-conquer'' methods, where the dataset is divided into blocks with local estimate computed for each data block, then the global estimate is obtained from a consensus of the local estimates. As theoretically and empirically demonstrated in \cite{YZhang2012}, the statistical optimality is retained for AVGM given enough information per block, \eg when $|\mathcal{Y}_b|$ is not too small. However, almost by definition complicated models that require large-scale dataset to learn may have at least subsets of local parameters $\theta_b$ that cannot be accurately estimated only based on $\mathcal{Y}_b$. Besides, randomly partitioning a dataset into blocks may introduce extra uncertainty within each block. What is more, in some real large-scale learning scenarios  the data are stored across a cluster in a distributed fashion, where each computing node may contain data with similar characteristics, \eg movie ratings or Ads-click log are recorded within a similar time period are stored in the same block, in such cases bias exists across different blocks that needs to be considered. And even for randomly partitioned dataset, the small-sample bias problem still exists as studied in \cite{YZhang2012, Scott2013}. To address the aforementioned problems, in the following sections we will focus on developing a series of probabilistic divide-and-conquer based methods as complement to their non-probabilistic counterparts.

%do we have more good motivations than what I wrote above?

\section{Probabilistic Divide-and-Conquer Methods}\label{proposed_frameworks}

The basic idea of our probabilistic methods is that, instead of finding a point estimate $\theta_b$ for each block $b$, we model $\theta_b$ as a latent (unobserved) random variable and consider a more general problem of finding a distribution $q(\theta_b)$ over all possible configurations of it. In the following we will first discuss modelling frameworks built on this idea and then discuss the corresponding learning algorithm that enables the probabilistic methods to share similar computational merits of their well-studied non-probabilistic counterparts in the large-scale setting.

\subsection{From a maximum entropy perspective}\label{MEM}
Perhaps the most straightforward way of formulating a empirical risk minimization problem with respect to a distribution is via the maximum entropy principle, which is well-studied in natural language processing literature. Under the maximum entropy principle, the problem becomes finding a distribution $\hat q(\theta_b)$, such that ($i$) the convex combination of local objective functions (\ref{avgm_local}) of block $b$ with respect to $\hat q(\theta_b)$, which is denoted as $\mathbb{E}_{\hat q(\theta_b)}[f(\theta_b)] = \int f(\theta_b)\hat q(\theta_b) d\theta_b$, is minimized; and ($ii$) at the same time the entropy of $\hat q(\theta_b)$, which is denoted as $\mathbb{H}[\hat q(\theta_b)] = -\int \hat q(\theta_b)\log  \hat q(\theta_b) d\theta$, is maximized. Mathematically the problem can be formulated as:
\begin{equation}\label{mem1}
\textstyle\hat q(\theta_b) =\argmin_{q(\theta_b)} -\mathbb{H}[q(\theta_b)] +  \mathbb{E}_{q(\theta_b) }[f(\theta_b)]
\end{equation}
After obtaining $\hat q(\theta_b)$, $\hat\theta$ is estimated as a (weighted) average of the mean of $\hat q(\theta_b)$ similar to AVGM: 
\begin{equation}\label{mem2}
\hat\theta = \textstyle\sum_{b=1}^Bw_b\mathbb{E}_{\hat q(\theta_b) }[\theta_b]/\sum_{b=1}^Bw_b
\end{equation}
In the following we refer (\ref{mem1}) and (\ref{mem2}) as the Maximum Entropy Mixture (MEM) method. The intuitive meaning of (\ref{mem1}) can be understood as by minimizing the negative entropy (maximizing the entropy), $q(\theta_b)$ is chosen to be the distribution that makes the least claim to being informed beyond minimizing the convex combination of local objective functions $\mathbb{E}_{q(\theta_b) }[f(\theta_b)]$. Note that based on the estimated set of distributions $\{\hat q(\theta_1), \ldots, \hat q(\theta_B)\}$, we can assess the quality of the estimator $\hat\theta$, \eg calculating the credible/confidence interval of $\hat\theta$ similar to the Bootstrap based methods in \cite{Kleiner2012}, but to relate our method to AVGM, here we focus on the case where the point estimate of $\hat\theta$ only of interest, which is taken to be a (weighted) average of the mean of the parameters $\theta_b$ as in (\ref{mem2}).

After some simple algebra, it can be shown that the optimal solution for (\ref{mem1}) has the following simple form
\begin{equation}\label{mem_solution}
\textstyle \hat q(\theta_b) = \frac{1}{Z}e^{-f(\theta_b)}
\end{equation}
here $Z = \int e^{-f(\theta_b)} d\theta_b$ is the normalization constant. Note that AVGM actually finds the maximum likelihood estimate of (\ref{mem_solution}) with respect to $\theta_b$, which suggests that AVGM is a special case of MEM.

%A different way to relate MEM to AVGM: Note that when $\hat q(\theta_b)$ is restricted to be a Dirac delta distribution with point mass at $\tilde\theta_b$, \eg $\hat q(\theta_b) = \delta_{\tilde\theta_b}(\theta_b)$, it's easy to show that $\tilde\theta_b = \hat\theta_b$, where the later corresponds to the empirical local minimizer of AVGM in (\ref{avgm})

\subsection{From a hierarchical modelling perspective}\label{HM}

One common feature of the AVGM and MEM methods is that, the local estimators are obtained only use data from the their own block, \ie $\theta_b$ only depends on $\mathcal{Y}_b$, and is independent of data from other blocks. This strategy leads to communication-efficient distributed algorithms, because different computing nodes only needs to communicates once when estimating the global parameter $\theta$. However, as discussed in the beginning of Section \ref{proposed_frameworks}, the information contained in one single block may not be sufficient to obtain accurate local estimator, and it could be helpful to share the statistical strength across data blocks when estimating the local parameters under the condition of not increasing too much communication cost.

%Is Bayesian hierarchical model particularly good at sharing statistics? Can we say something about it as the motivation to use it?

In the next we propose a method that facilitates the sharing of statistical strength among data blocks, through hierarchical modelling and Bayesian integration of prior information. Similar to MEM, we model $\theta_b$ as latent random variable and are interested in finding the distribution $q(\theta_b)$. Besides, we build a hierarchical model by placing a prior distribution $p(\theta_b | \theta)$ over each latent random variable $\theta_b$, such that $\theta_b$ is dependent on the global parameter $\theta$ \emph{a priori}, but is conditionally independent of all other latent variables variables given $\theta$. To incorporate the prior and hierarchical information into the empirical risk minimization framework, together with the convex combination of local objective functions, the distance between $q(\theta_b)$ and the prior distribution $p(\theta_b|\theta)$ is minimized, with the distance between distributions measured by the Kullback-Leibler (KL) divergence. The proposed hierarchical model (referred to as HM in the following sections) can be summarized as follows
\begin{equation}\label{hm}
\textstyle \min_{\theta, q(\cdot)} ~ \sum_{b=1}^B f(q(\theta_b), \theta) + h(\theta)
\end{equation}
where $q(\cdot) = \{q(\theta_1), \ldots, q(\theta_B)\}$ and the local objective function is defined as
\begin{equation}\label{local_obj}
\textstyle f(q(\theta_b), \theta)  = \mathbb{D}[q(\theta_b)||p(\theta_b|\theta)] + \mathbb{E}_{q(\theta_b)}[l(\mathcal{Y}_b; \theta_b)]
\end{equation}
where $\mathbb{D}[q(\theta_b)||p(\theta_b|\theta)] = \int q(\theta_b)\log\frac{q(\theta_b)}{p(\theta_b|\theta)}d\theta_b$ is the KL divergence between $q(\theta_b)$ and $p(\theta_b|\theta)$. Note that MEM method can be recovered by letting $p(\theta_b|\theta)$ to be an uninformative (possibly improper) prior. 

Through the introduced hierarchy between data block $\mathcal{Y}_b$, $\theta_b$ and $\theta$, the induced conditional independence suggests that $\mathcal{Y}_b$ and $\theta_b$ still can be stored independent of other blocks in a distributed fashion, while the dependency between all latent variables $\{\theta_b\}_{b=1\ldots B}$ and global parameter $\theta$ via prior distribution $p(\theta_b|\theta)$ facilitates the sharing of statistical strength across different blocks. Besides, the formulation of HM naturally leads us to a generalized Expectation-Maximization (gEM)\footnote{The reason we call it generalized EM is because some loss function $l(\mathcal{Y}_b; \theta_b)$ can't be interpreted as likelihood function, as we will discuss later.} algorithm to estimate $\theta$ and $q(\theta_b)$ within the same framework, instead of two independent stages as in AVGM and MEM, as we discuss in the following.

The gEM algorithm proceeds by iteratively applying two steps: the E-step finds a distribution $q(\theta_b)$ that minimizes objective function (\ref{hm}) while fixing $\theta$, and evaluate the expectation and KL-divergence in (\ref{local_obj}) with respect to $q(\theta_b)$; and the M-steps in turn finds an estimator $\theta$ that minimizes (\ref{hm}) while fixing $q(\theta_b)$. More concretely, in the $k^{th}$ iteration, the E-step updates $q^{k+1}(\theta_b)$ by minimizing $f(q(\theta_b), \theta^k)$, which can be shown has the following optimal solution: 
\begin{equation}\label{hm_e_step_sol}
\textstyle q^{k+1}(\theta_b) = \frac{1}{Z}p(\theta_b|\theta^k)e^{-l(\mathcal{Y}_b; \theta_b)}
\end{equation}
where $Z = \int_{\theta_b}p(\theta_b|\theta^k)e^{-l(\mathcal{Y}_b; \theta_b)}d\theta_b$ is the normalization constant.  And given $q^{k+1}(\theta_b)$, the M-step consists of updating $\theta^{k+1}$ by solving the following problem
\begin{equation}\label{hm_m_step}
\textstyle\min_{\theta}~\sum_{b=1}^B\mathbb{E}_{q^{k+1}(\theta_b)}[-\log p(\theta_b|\theta)] + h(\theta)
\end{equation}
%am I now stating the obvious?

Note that minimizing objective function (\ref{local_obj}) with respect to $q(\theta_b)$ in the E-step is closely related to the Bayes theorem: $p(\theta_b|\mathcal{Y}_b, \theta) = \frac{p(\theta_b|\theta)p(\mathcal{Y}_b|\theta_b)}{\int p(\theta_b|\theta)p(\mathcal{Y}_b|\theta_b)d\theta_b}$ with $p(\theta_b|\theta)$ and $p(\mathcal{Y}_b|\theta_b)$ representing the prior distribution density and likelihood function respectively. Zellner \yrcite{Zellner1988} shows that the Bayesian posterior density $p(\theta_b|\mathcal{Y}_b, \theta)$ can equivalently be found by solving the following minimization problem:
\begin{equation}\label{bayes_rule}
\textstyle \min_{q(\theta_b)} ~\mathbb{D}[q(\theta_b)||p(\theta_b|\theta)] + \mathbb{E}_{q(\theta_b)}[-\log p(\mathcal{Y}_b|\theta_b)]
\end{equation}
By comparing (\ref{local_obj}) against (\ref{bayes_rule}), we see that if $p(\mathcal{Y}_b|\theta_b) \propto e^{-l(\mathcal{Y}_b; \theta_b)}$, \ie when $l(\mathcal{Y}_b; \theta_b)$ can be interpreted as a negtive log-likelihood function, then $q^{k+1}(\theta_b)$ in (\ref{hm_e_step_sol}) is exactly the posterior distribution $p(\theta_b|\mathcal{Y}_b, \theta^k)$, \ie $q^{k+1}(\theta_b) = p(\theta_b|\mathcal{Y}_b, \theta^k)$. In this case, the gEM algorithm reduces to the classic EM algorithm for statistical models. In many scenarios the loss function $l(\mathcal{Y}_b; \theta_b)$ does have a probabilistic interpretation, \eg  squared $\ell_2$ and $\ell_1$ loss functions are proportional to the  Gaussian and Laplace negative log-likelihood function respectively, however, some important loss functions doesn't have a probabilistic counterpart, \eg the hinge loss function for max-margin methods.

Related frameworks of (\ref{hm}) and (\ref{local_obj}) can be found in the literature for different applications, some are known as the minimum relative entropy method \cite{Jaakkola1999, Zhu2012} for max-margin based discriminative learning tasks, or the posterior regularization method \cite{Ganchev2010} and constrained Bayesian inference \cite{Koyejo2013} for incorporating constraint into posterior inference.

%need a better section title here
\subsection{Modelling the local bias}\label{pADMM}

\begin{algorithm}[tb]
   \caption{pADMM}
   \label{alg:pADMM}
\begin{algorithmic}
   \STATE {\bfseries Initialize:} $\theta^0, \rho^0, \{q^0(\theta_b), \lambda_b^0, \gamma_b^0\}_{b=1\ldots B}$
   \FOR{$k= 0, 1, 2, \ldots$}
   \STATE E-step: 
   \FOR{$b=1$ {\bfseries to} $B$ in parallel}
   \STATE Update $q^{k+1}(\theta_b), \lambda_b^{k+1}$ based on (\ref{padmm_primal_sol}), (\ref{padmm_dual_sol}).
   \STATE Update $\gamma_b^{k+1}$ based on (\ref{em_bayes}).
   \ENDFOR
   \STATE M-step:
   \STATE Update $\theta^{k+1}$ by solving (\ref{padmm_m_step}) and $\rho^{k+1}$ based on (\ref{em_bayes})
   \ENDFOR
\end{algorithmic}
\end{algorithm}

As discussed at the beginning of Section \ref{proposed_frameworks}, in the large-scale setting the dataset is (pre-)partitioned into blocks, and consequently the bias that is absent in the original dataset may present in some of the blocks, which may in turn lead to biased local estimator. Such problem is addressed with the bootstrap/jackknife bias correction method in the AVGM setting as a post-processing step, where the local estimates are shifted to avoid an accumulation of biases in the global estimate \cite{YZhang2012, Scott2013}, and similar idea can be applied to the MEM method.

In this section we address the biased local estimator problem by explicitly model it as an integrated part of the HM method. The basic idea is to constrain the latent random variables $\theta_b$ to be consistent with each other in expectation, which is equivalent to explicitly constraining $q(\theta_b)$ to share the same mean across all blocks. Note that although $q(\theta_b)$s' centres are now fixed to be the same, they are still flexible enough as their shapes, \eg measured by skewness and kurtosis, still can vary. Mathematically the optimization problem becomes
\begin{equation}\label{padmm}
\begin{gathered}
\textstyle\min_{\theta, q(\cdot)} \quad\textstyle\sum_{b=1}^B f(q(\theta_b), \theta) + h(\theta)\\
\textstyle\st \quad \mathbb{E}_{q(\theta_b)}[\theta_b] = \theta, ~ b = 1, \ldots, B
\end{gathered}
\end{equation}
where $f(q(\theta_b), \theta)$ is the local objective function defined in (\ref{local_obj}). We name this method pADMM, which stands for probabilistic Alternating Direction Method of Multipliers, as we will show later that pADMM is closely related to Alternating Direction Method of Multipliers (ADMM). In order to make their connection transparent, in the following we focus on a special case when $p(\theta_b|\theta)$ is Gaussian:
\begin{equation}\label{gaussian_prior}
\textstyle p(\theta_b|\theta) = \mathcal{N}(\theta_b | \theta, (\rho\gamma_b)^{-1}\Imat)
\end{equation}
with mean $\theta$ and isotropic covariance matrix $(\rho\gamma_b)^{-1}\Imat$, where $\Imat$ denotes the identity matrix. In the following $\rho$ and $\gamma_b$ will be referred to as the global and local precision parameter respectively. Later in this section we will show $\rho$ and $\gamma_b$ can be learned from data, which empirically will lead to faster convergence of the algorithm, and thus less communications between computing nodes.

In the next we apply the gEM algorithm discussed in Section \ref{HM} to pADMM. With the Gaussian assumption in (\ref{gaussian_prior}), the local objective function $f(q(\theta_b), \theta)$ becomes (with constants independent of $q(\theta_b)$ and $\theta$ ignored) :
\begin{equation}\label{local_objective_gaussian}
f(q(\theta_b), \theta) = \mathbb{E}_{q(\theta_b)}\left[\frac{\rho\gamma_b}{2}||\theta_b - \theta||_2^2 + l(\mathcal{Y}_b; \theta_b)\right]
\end{equation}
And we have the following Lagrangian for each local optimization problem
\begin{equation}\label{padmm_lagrangian}
\begin{gathered}
\textstyle \min_{q(\theta_b), \lambda_b} ~ f(q(\theta_b), \theta) + \lambda_b\cdot(\mathbb{E}_{q(\theta_b)}[\theta_b]- \theta)
\end{gathered}
\end{equation}
Where $f(q(\theta_b), \theta)$ is defined in (\ref{local_objective_gaussian}), $\lambda_b$ is the Lagrange dual variable and $\cdot$ represents the dot product. The dual of (\ref{padmm_lagrangian}) can be readily derived, and in the E-step we propose to solve problem (\ref{padmm_lagrangian}) via dual ascent, \eg the dual problem is solved using gradient ascent. With fixed dual variable and global parameter, the updating equation of $q(\theta_b)$ is:
\begin{equation}\label{padmm_primal_sol}
\textstyle q^{k+1}(\theta_b) = \frac{1}{Z}e^{-l(\mathcal{Y}_b; \theta_b) -  \frac{\rho\gamma_b}{2}||\theta_b-\theta^k + \mu^k_b||^2_2}
\end{equation}
where $Z = \int e^{-l(\mathcal{Y}_b; \theta_b) -  \frac{\rho\gamma_b}{2}||\theta_b-\theta^k + \mu^k_b||^2_2} d\theta_b$ is the normalization constant. $\mu^k_b = \frac{1}{\rho\gamma_b}\lambda^k_b$ is the scaled dual variable, which acts in a similar role to the Bootstrap adjustment in \cite{YZhang2012}, and pADMM reduces to HM by fixing $\lambda^k_b = 0$. Inspired by ADMM (will be discussed in Section \ref{ADMM}), the dual variable $\lambda_b$ is updated using gradient ascent with step size $\rho\gamma_b$:
\begin{equation}\label{padmm_dual_sol}
\lambda^{k+1}_b = \lambda^k_b + \rho\gamma_b(\mathbb{E}_{q^{k+1}(\theta_b)}[\theta_b] - \theta^k)
\end{equation}
Finally, given $q^{k+1}(\theta_b)$ and $\lambda^{k+1}_b$ obtained from the E-step for each local block $b$, the M-step updates $\theta^{k+1}$ by solving the following minimization problem:
\begin{equation}\label{padmm_m_step}
\textstyle \min_{\theta} ~\sum_{b=1}^B\frac{\rho\gamma_b}{2}||\theta-\mathbb{E}_{q^{k+1}(\theta_b)}[\theta_b] + \mu^{k+1}_b||^2_2 + h(\theta)
\end{equation}
where $\mathbb{E}_{q^{k+1}(\theta_b)}[\theta_b]$ is simply the mean of $q^{k+1}(\theta_b)$, and recall that $\mu^{k+1}_b$ is the scaled dual variable. Optimization problem of form (\ref{padmm_m_step}) generally can be solved efficiently by proximal gradient methods \cite{Parikh13}, and as one example, when $h(\theta) = \beta(\alpha||\theta||_1 + (1-\alpha)||\theta||^2_2)$ corresponds the elastic-net regularizer with parameter $0\le\alpha\le 1$ and $\beta\ge0$, the updating equation for $\theta^{k+1}$ is
\begin{equation}\label{elastic_net}
 \theta^{k+1} = \frac{S(\textstyle\sum_{b=1}^B\rho\gamma_b\mathbb{E}_{q^{k+1}}[\theta_b], \beta\alpha )}{\sum_{b=1}^B\rho\gamma_b + \beta(1-\alpha)}
\end{equation}
Where $S(a, b) = \mbox{sign}(a)(|a|-b)_{+}$ is the soft thresholding operator \cite{Friedman2010}. Note that (\ref{elastic_net}) resembles the weighted average strategy in (\ref{avgm}), however, instead of fixing $w_b$ to $1/B$ or proportional to $|\Omega_b|$, the weight corresponds to the product of global and local precision parameters $\rho\gamma_b$. In the next we propose to update $\rho$ and $\gamma_b$ in the M-step, this can be done by minimizing (\ref{padmm}) with respect to $\rho$ and $\gamma_b$ in turn. Denoting the expected squared residual as $r^{k+1} = \mathbb{E}_{q^{k+1}(\theta_b)}[||\theta_b^{k+1}-\theta^{k+1}+\mu_b^{k+1}||_2^2]$, the updating equations for $\rho^{k+1}$ and $\gamma_b^{k+1}$ can be shown to be
\begin{equation}\label{em_bayes}
\begin{gathered}
\textstyle \rho^{k+1} = \frac{BP}{\sum_{b=1}^B\gamma_b^kr^{k+1}}, \quad \gamma_b^{k+1} = \frac{P}{\rho^{k+1}r^{k+1}}
\end{gathered}
\end{equation}
Where $P$ denotes the dimension of $\theta_b$. pADMM method is summarized in Algorithm \ref{alg:pADMM}, note that  by setting $\mu_b = 0$, pADMM reduces to HM. As a side note, for the HM method if $\gamma_b^k$ is fixed and $\rho^k$ is setting to be an increasing sequence, \eg $\rho^1 < \rho^2 < \ldots$ and $\lim_{k\to\infty}\rho^{k} = \infty$, it reduces to the penalty methods \cite{Bertsekas1989, Bertsekas1996} if $q(\theta_b)$ is constrained to be a Dirac delta function (as MEM reduces to AVGM).

\subsection{Full Bayesian treatment}\label{Full_Bayesian}
Although the frameworks we proposed, \ie MEM, HM and pADMM, are motivated by the optimization problem (\ref{general_obj}), they can naturally be extended to full Bayesian frameworks. As shown by equation (\ref{bayes_rule}), the gEM algorithms proposed for HM and pADMM reduces to the classic EM algorithms when $l(\mathcal{Y}_b; \theta_b)$ corresponds to a negative log-likelihood function. Based on this observation, if the global parameter $\theta$ is also modelled as a random variable, and the regularization function $h(\theta)$ in (\ref{general_obj}) is replaced by a prior distribution, posterior inference can be performed on $\theta$, and then we have a full Bayesian model. However, this involves the design of efficient inference algorithm for Bayesian posterior computation and is beyond the scope of this work. \cite{Scott2013, Neiswanger2013} represent two recent work in this direction, although their frameworks are both based on the AVGM setting.


\section{Practical Issues of the gEM Algorithm}
One core issue in making the gEM practical in large-scale setting is to evaluate the expectations efficiently in the E-step, \eg $\mathbb{E}_{q(\theta_b)}[\theta_b]$, which in turn generally requires an analytical expression of the distribution $q(\theta_b)$, or at least being able to draw samples from it. And given the expectations found in the E-step, efficient optimization algorithms, \eg the proximal gradient method or stochastic gradient based methods, becomes applicable for the M-step. Consequently, the first key technical component in our implementation is the adoption of the variational methods \cite{Wainwright2008} in the E-step, which transforms the gEM algorithm to the variational EM algorithm, and if we extend our method to be full Bayesian as discussed in Section \ref{Full_Bayesian}, gEM becomes the variational Bayesian inference algorithm \cite{Bishop2006}.

As discussed in Section \ref{MEM}-\ref{pADMM}, for MEM, HM and pADMM the distribution $q(\theta_b)$ of interest is found by minimizing the local objective function (\ref{local_obj}), which could be problematic when the expectation $\mathbb{E}_{q(\theta_b)}[l(\mathcal{Y}_b; \theta_b)]$ has no analytical expression. Two types of variational methods, commonly referred to as local and global variational methods in the literature \cite{Bishop2006}, can be applied here. The local variational methods first find an upper bound of the loss function $\tilde l(\mathcal{Y}_b; \theta_b; \xi_b) \ge  l(\mathcal{Y}_b; \theta_b)$, then the expectation of the upper bound $\mathbb{E}_{q(\theta_b)}[\tilde l(\mathcal{Y}_b; \theta_b; \xi_b)]$ is minimized to find $q(\theta_b)$, with $\xi_b$ representing some variational parameters that can be optimized to tighten the gap between $\tilde l(\mathcal{Y}_b; \theta_b; \xi_b)$ and $l(\mathcal{Y}_b; \theta_b)$. Local variational method are well studied for various types of loss functions and likelihood functions, a few examples include \cite{Zhu2012} for hinge loss function, \cite{Jaakkola2000, Khan2010} for logistic/soft-max loss function, and \cite{Khan2013, Wang2013} for general non-conjugate likelihood functions. 

The global variational methods, on the other hand, restrict the range of $q(\theta)$ over which the optimization is performed, \eg the mean field methods restrict $q(\theta)$ to take factorized forms. Mean field based variational methods can be found in many Bayesian hierarchical models with latent variables and intractable posterior distributions \cite{Wainwright2008}. Here we propose to use mean field variational methods from a slightly different perspective, that by restricting $q(\theta) = \prod_i q(\theta_i)$ to be fully factorized across its components, the variational methods proceeds by updating each scalar based density $q(\theta_i)$ in turn which avoids computing high dimensional quantities, \eg the covariance matrix of $\theta$. This type of coordinate-wise algorithm is surprisingly efficient and scalable in large-scale applications, their non-probabilistic counterpart in optimization literature, the coordinate descent algorithms which regained much interest in statistics and ML community since \cite{Friedman2008, Banerjee2008}, are now widely applied in various applications \cite{Friedman2010, Bradley2011, Yuan2012, Yu2013}.

Although sampling based methods, \eg MCMC and importance re-sampling, generally doesn't need to evaluate the normalization constant $Z$ for $q(\theta_b)$ explicitly and can readily be applied to our framework, the reason we prefer variational method to sampling based method is two-fold, ($i$) It is generally more difficult to monitor the convergence of a sampling based method than deterministic variational method;  and ($ii$) our preliminary experiment suggests that in the high-dimensional and large-scale setting MCMC tends to take more iterations to reach a reasonable solution than variational method, and importance re-sampling tends to collapse to a single point.

% I discuss ADMM here partly because ADMM will converge, and I show ADMM actually finds a point estimate of pADMM, in this way I don't need to show some convergence result for pADMM, which is difficult to me. 

\section{Related Distributed Learning Methods}

\subsection{Distributed learning via ADMM}
\label{ADMM}
ADMM is gathering much attentions in a variety of applications in recent years, and a comprehensive survey on ADMM, especially its application on distributed learning, can be found in \cite{Boyd10}. Here we only summarize important aspects of ADMM. First note that we can re-write (\ref{general_obj}) into the following equivalent problem:
\begin{equation}\label{admm_obj}
\begin{gathered}
\min_{\theta, \theta_b} ~ \textstyle\sum_{b=1}^Bl(\mathcal{Y}_b,\theta_b) + h(\theta)\\
\st ~ \theta_b - \theta = 0, ~ b = 1,\ldots,B
\end{gathered}
\end{equation}
The ADMM formulation for the problem (\ref{admm_obj}) can be derived directly from the following augmented Lagrangian
\begin{equation}\label{admm_lag_global}
\textstyle L_\rho(\{\theta_b, \lambda_b\}_{b=1}^B, \theta) =  \sum_{b=1}^BL_\rho(\theta_b, \lambda_b, \theta) + h(\theta)
\end{equation}
Here $L_\rho(\theta_b, \lambda_b, \theta)$ is the local augmented Lagrangian:
\begin{equation}\label{admm_lag_local}
\textstyle L_\rho(\theta_b, \lambda_b, \theta)  = l(\mathcal{Y}_b, \theta_b) + \lambda_b\cdot(\theta_b - \theta) + \frac{\rho}{2}||\theta_b - \theta||_2^2
\end{equation}
Where $\rho \ge 0$ is the tuning parameter of the augmented Lagrangian. ADMM proceeds by iteratively updating $\theta_b, \lambda_b$ for each block $b$ independently and $\theta$:
\begin{equation}\label{admm_update_local}
\textstyle \theta_b^{k+1} = \argmin_{\theta_b}~L_\gamma(\theta_b, \lambda_b^k, \theta^k)
\end{equation}
\begin{equation}\label{admm_update_lag}
\textstyle \lambda_b^{k+1} = \lambda_b^k + \rho(\theta_b^{k+1} - \theta^{k})
\end{equation}
\begin{equation}\label{admm_update_global}
\theta^{k+1} = \argmin_\theta ~\textstyle\sum_{b=1}^B\frac{\rho}{2}||\theta_b^{k+1} - \theta + \mu_b^{k+1}||^2_2 + h(\theta)
\end{equation}
Note that in the gradient ascent update in (\ref{admm_update_lag}) the step size is chosen to be $\rho$. The motivation behind this choice is explained by Boyd el al. \yrcite{Boyd10}, that by using $\rho$ as the step size, the iterate
$\theta_b^{k+1}, \lambda_b^{k+1}$ in (\ref{admm_update_local} - \ref{admm_update_lag}) is dual feasible, and as the ADMM proceeds the primal residual $\theta_b -\theta$ converges to zero, together with the dual feasible condition the procedure (\ref{admm_update_local} - \ref{admm_update_global}) will yield optimal solution $\hat\theta$. 

Now comparing updating equations for ADMM in (\ref{admm_update_local} - \ref{admm_update_global}) against that of for pADMM (\ref{local_objective_gaussian}, \ref{padmm_primal_sol} - \ref{padmm_m_step}), we see that the gEM algorithm for pADMM can be thought as a probabilistic version of ADMM, hence the name. On the other hand, ADMM can be thought as a way of finding a point estimate for pADMM, with pADMM reduces to ADMM when $q(\theta_b) = \delta_{\hat\theta_b}(\theta_b)$ becomes a Dirac delta distribution. This again resembles the connections between AVGM and MEM, and HM and penalty methods. Note that carefully tuning the parameter $\rho$ is one caveat of reaching fast convergence of ADMM, which is commonly done by cross-validation, which could be particularly computationally expensive in the large-scale setting as one need multiple passes over the whole dataset. However, as mentioned before the fast convergence is important for iterative algorithms in the distributed learning scenario as less number of iterations means less expensive communication among computing nodes, and this is particularly critical when the dimension of the parameter is high. Although the same problem exists for HM and pADMM, we will show empirically that by updating $\rho$ and $\gamma_b$ as in (\ref{em_bayes}), pADMM mitigates the dilemma ADMM faces by having comparable or faster convergence against a well-tuned ADMM.
 

\subsection{Distributed (sub-)gradient method}
We have been focused on the "divide-and-conquer" type of frameworks thus far, \eg AVGM, MEM, HM, ADMM and pADMM, and in this section we briefly discuss another type of distributed learning algorithm, the distributed sub-gradient method, which will serve as an important baseline when we compare different distributed algorithm empirically through experiments. The distributed sub-gradient method for optimization problem (\ref{general_obj}) is straightforward, where in each iteration the sub-gradients $\partial l(\mathcal{Y}_b; \theta)$ are computed independently for each block $b$, and these separate sub-gradients are then summed up to compute the exact global sub-gradients $\textstyle\sum_{b=1}^B\partial l(\mathcal{Y}_b; \theta)$, which are used to perform the optimization step and update the parameter $\theta$ received by all $B$ blocks for the next iteration's sub-gradients computation. The distributed sub-gradient method can be readily expressed under the MapReduce paradigm \cite{Dean2004} and is guaranteed to find the (local) optimal solution, however, it requires relatively many iterations before convergence which in turn requires frequent communications among nodes in the cluster. This can be a drawback when the dimensionality of the data and the parameter is high, as communication can be prohibitively expensive. On the other hand, the asymptotically optimal AVGM and MEM based methods represent another extreme of the distributed learning framework, where communication only happens once, but the solution might not be optimal when each data block contains not enough information. At last, ADMM and the proposed HM, ADMM methods lies in the middle of the above two frameworks, which require moderately less number of iterations than distributed sub-gradient methods and may overcome some difficulties of AVGM in certain scenarios.


\section{Empirical Study}
With the variational gEM algorithm described above, we solve two challenging problems, the non-smooth $\ell_1$-regularized logistic regression, and the non-convex low-rank matrix completion, with our proposed distributed learning methods. In the following we first introduce our experimental setting.

\subsection{Experimental setting}

MPI has no fault tolerance and doesn't scale very well to large clusters, Hadoop requires many disk read/write, which may lead to distract our attention on analysing the reason caused the different convergence behaviour of different methods, and Hadoop is too slow in iterative algorithms. (Not finished yet)

\subsection{Distributed $\ell_1$-regularized logistic regression}
In logistic regression the each training data $y_n$ contains two parts, $y_n = \{l_n, x_n\}$ where $l_n \in \{-1, +1\}$ represents the label and $x_n \in \mathbb{R}^P$ represents the feature vector with dimension $P$, and parameter $\theta\in\mathbb{R}^P$ also has the same dimension. The loss function in (\ref{general_obj}) is $l(\mathbb{Y}_b; \theta) = \sum_{n\in\Omega_b}\log(1+\exp(-l_n(x_n\cdot\theta)))$, and the regularization function is $h(\theta) = C||\theta||_1$, where $C$ is a tuning parameter controls the sparsity of the parameter $\theta$. And we assume the prior distribution $p(\theta_b|\theta)$ is Gaussian as in (\ref{gaussian_prior}) for HM and pADMM. 

In this case distribution $q(\theta_b)$ doesn't readily have an analytical form, as the normalization constant $Z$ in (\ref{mem_solution}) for MEM, (\ref{hm_e_step_sol}) for HM, and (\ref{padmm_primal_sol}) for pADMM are all intractable. And in this work we will use the Laplace variational method \cite{Wang2013} and the Bohning bound based variational method \cite{Khan2010} as they work favourably compared to other variational methods both computationally and efficaciously in our preliminary experiments.

\paragraph{Data description} Our data for off-line experiments consist of 60 days of advertising event logs for a major social network site. We use the first 45 days of data as training and the last 15 days data as test. Both the training and test sets consist of hundreds of millions of events. We use thousands of features for both user, item (ad campaign), and the context feature. For instance, the ad campaign features include n-grams, categories, advertiser characteristics, among others. The context features include time of day, day of week, the page id that the ad is shown, and the format id that the ad is placed with. We use mutual information and the minimum support criteria to do feature selection on these features and the two-way interactions between all pairs. Finally we obtain a cold-start feature set that contains roughly 100K binary features.


\subsection{Distributed low-rank matrix completion}
In our notation, $\mathcal{Y} = \{y_n, n\in\Omega\}$ corresponds to observed entries of a matrix $\Ymat$ of dimension $I \times J$, $n = (i,j)$ indexes each entry, and $\Omega$ is a subset of $\{1, \ldots, I\}\otimes\{1,\ldots, J\}$ denoting the indexes of the observed entries in $\Ymat$ with $\otimes$ denoting the Cartesian product. And similarly $\theta \in \mathbb{R}^{I\times J}$ denotes the model parameters of interest. The loss function is quadratic $l(\mathcal{Y}; \theta) = \sum_{(i,j)\in\Omega}(y_{i,j} - \theta_{i,j})^2$, and the regularization functions $h(\theta)$ we consider here are the nuclear norm and the $\gamma_2$-norm (also know as the max-norm) \cite{Srebro2004}. We assume $\theta$ has rank at most $K$, in which case $\theta$ can be explicitly written as $UV'$ where $U \in \mathbb{R}^{I\times K}$ and $V \in \mathbb{R}^{J \times K}$. Such approximation transforms the low-rank matrix completion a non-convex problem, with objective function now can be written as
\begin{equation}\label{mf_obj}
\textstyle f(U,V) = \sum_{(i,j)\in\Omega}(y_{i,j} - U_{i\cdot}V_{j\cdot}')^2 + h(U) + h(V)
\end{equation}
Note that the nuclear norm and $\gamma_2$-norm on $\theta$ can be equivalent formulated in terms of $U$ and $V$, such that the nuclear norm can be written as $h(U) = ||U||_F^2 := \sum_{i}\sum_kU_{ik}^2$, and $\gamma_2$-norm can be written as $h(U) = ||U||_{2,\infty}^2 := \max_i(\sum_k U_{ik}^2)$ \cite{Recht2010}. The optimization techniques for both norms that could be used in the M-step of the gEM algorithm can be found at \cite{Salakhutdinov2007, Lee2010}. In the large-scale setting, we randomly partition the observed data $\mathcal{Y}$ into $B$ blocks similar to the two-way random partition method in \cite{Recht2011}, with each block corresponds to a sub-matrix of size approximately $\frac{I}{B_r} \times \frac{J}{B_c}$, such that $B_rB_c = B$, and with parameters $U$ and $V$ conformally partitioned into $B_r$ and $B_c$ blocks respectively. Consequently, the pADMM formulation of the low-rank matrix completion problem is
\begin{equation}\label{mf_padmm}
\begin{gathered}
\textstyle \min_{U,V, q(\cdot)}~ \sum_{b=1}^B f(U_b,V_b, U, V) + h(U) + h(V)\\
\st ~ \mathbb{E}_{q(U_b)} = U_b, ~ \mathbb{E}_{q(V_b)} = V_b \\
\end{gathered}
\end{equation}


\begin{table}\label{mf_alg_comp}
\footnotesize
\caption{Comparison of memory and network usage for two types of matrix factorization algorithms}
\begin{tabular}{c|c|c}
~  &  network per epoch & memory per core\\
\hline
ALS/CD & $\mathcal{O}( (M+N) K B)$  & $\mathcal{O}(MK+NK + \frac{|\Omega|}{B})$  \\
Proposed & $\mathcal{O}( MK B_{c} + NK B_{r})$ & $\mathcal{O}(\frac{MK}{B_{r}}+\frac{NK}{B_{c}} + \frac{|\Omega|}{B})$ \\
\end{tabular}
\end{table}


\begin{table}\label{mf_datasets}
\caption{ Statistics and parameters for each dataset}
\begin{tabular}{c|ccc}
~ & Netflix &  KDDCup2011 & Synthetic\\
\hline
M & 2,649,429 & 1,000,990 & 1,000,000\\
N & 17,770 & 624,961 & 1,000,000 \\
$|\Omega|$ & 99,072,112 & 252,800,275 & 1,983,749,510 \\
$|\bar\Omega|$ & 1,408,395 & 4,003,960 & 23,328,835  \\
$K$ & 30 & 50 & 30\\
$\lambda$ & 0.05 & 1 & 0.001
\end{tabular}
\end{table}



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\newpage
\bibliography{icml2014}
\bibliographystyle{icml2014}

\end{document} 
