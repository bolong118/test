%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2014 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{calc}
\usepackage[T1]{fontenc}
\usepackage{xspace}
\usepackage{amsmath,amssymb}
\usepackage{amsthm,amscd,amsfonts}
\usepackage{bm}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2014}

\newenvironment{packed_enum}{
\begin{enumerate}
  \setlength{\itemsep}{1pt}
  \setlength{\parskip}{0pt}
  \setlength{\parsep}{0pt}
}{\end{enumerate}}

\def\x{{\bm x}}
\def\z{{\bm z}}
\def\y{{\bm y}}
\def\s{{\bm s}}
\def\v{{\bm v}}
\def\n{{\bm n}}
\def\L{{\cal L}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\minimize}{minimize}
\DeclareMathOperator*{\st}{subject~to}
\newcommand{\beq}{\begin{equation}}
\newcommand{\eeq}{\end{equation}}
\newcommand{\beqs}{\begin{eqnarray}}
\newcommand{\eeqs}{\end{eqnarray}}
\newcommand{\barr}{\begin{array}}
\newcommand{\earr}{\end{array}}

\newcommand{\Rc}[0]{\ensuremath{\mathcal{R}}\xspace}
\newcommand{\Nc}[0]{\ensuremath{\mathcal{N}}\xspace}
\newcommand{\Gc}[0]{\ensuremath{\mathcal{G}}\xspace}
\newcommand{\Dc}[0]{\ensuremath{\mathcal{D}}\xspace}
\newcommand{\Oc}[0]{\ensuremath{\mathcal{O}}\xspace}
\newcommand{\Wc}[0]{\ensuremath{\mathcal{W}}\xspace}
\newcommand{\E}[0]{\ensuremath{\mathbb{E}}\xspace}

\newcommand{\zerov}[0]{\ensuremath{{\bf 0}}\xspace}
\newcommand{\onev}[0]{\ensuremath{{\bf 1}}\xspace}

\newcommand{\ie}[0]{\emph{i.e., }}
\newcommand{\ea}[0]{\emph{et al. }}
\newcommand{\eg}[0]{\emph{e.g., }}
\newcommand{\cf}[0]{\emph{cf. }}
\newcommand{\etc}[0]{\emph{etc.}}

\newcommand{\Amat}[0]{\ensuremath{{\bf A}}\xspace}
\newcommand{\Bmat}[0]{\ensuremath{{\bf B}}\xspace}
\newcommand{\Cmat}[0]{\ensuremath{{\bf C}}\xspace}
\newcommand{\Dmat}[0]{\ensuremath{{\bf D}}\xspace}
\newcommand{\Emat}[0]{\ensuremath{{\bf E}}\xspace}
\newcommand{\Fmat}[0]{\ensuremath{{\bf F}}\xspace}
\newcommand{\Gmat}[0]{\ensuremath{{\bf G}}\xspace}
\newcommand{\Hmat}[0]{\ensuremath{{\bf H}}\xspace}
\newcommand{\Imat}[0]{\ensuremath{{\bf I}}\xspace}
\newcommand{\Jmat}[0]{\ensuremath{{\bf J}}\xspace}
\newcommand{\Kmat}[0]{\ensuremath{{\bf K}}\xspace}
\newcommand{\Lmat}[0]{\ensuremath{{\bf L}}\xspace}
\newcommand{\Mmat}[0]{\ensuremath{{\bf M}}\xspace}
\newcommand{\Nmat}[0]{\ensuremath{{\bf N}}\xspace}
\newcommand{\Omat}[0]{\ensuremath{{\bf O}}\xspace}
\newcommand{\Pmat}[0]{\ensuremath{{\bf P}}\xspace}
\newcommand{\Qmat}[0]{\ensuremath{{\bf Q}}\xspace}
\newcommand{\Rmat}[0]{\ensuremath{{\bf R}}\xspace}
\newcommand{\Smat}[0]{\ensuremath{{\bf S}}\xspace}
\newcommand{\Tmat}[0]{\ensuremath{{\bf T}}\xspace}
\newcommand{\Umat}[0]{\ensuremath{{\bf U}}\xspace}
\newcommand{\Vmat}[0]{\ensuremath{{\bf V}}\xspace}
\newcommand{\Wmat}[0]{\ensuremath{{\bf W}}\xspace}
\newcommand{\Xmat}[0]{\ensuremath{{\bf X}}\xspace}
\newcommand{\Ymat}[0]{\ensuremath{{\bf Y}}\xspace}
\newcommand{\Zmat}[0]{\ensuremath{{\bf Z}}\xspace}

\newcommand{\1}[0]{\ensuremath{\boldsymbol{1}}\xspace}
\newcommand{\av}[0]{\ensuremath{\boldsymbol{a}}\xspace}
\newcommand{\bv}[0]{\ensuremath{\boldsymbol{b}}\xspace}
\newcommand{\cv}[0]{\ensuremath{\boldsymbol{c}}\xspace}
\newcommand{\dv}[0]{\ensuremath{\boldsymbol{d}}\xspace}
\newcommand{\ev}[0]{\ensuremath{\boldsymbol{e}}\xspace}
\newcommand{\fv}[0]{\ensuremath{\boldsymbol{f}}\xspace}
\newcommand{\gv}[0]{\ensuremath{\boldsymbol{g}}\xspace}
\newcommand{\hv}[0]{\ensuremath{\boldsymbol{h}}\xspace}
\newcommand{\iv}[0]{\ensuremath{\boldsymbol{i}}\xspace}
\newcommand{\jv}[0]{\ensuremath{\boldsymbol{j}}\xspace}
\newcommand{\kv}[0]{\ensuremath{\boldsymbol{k}}\xspace}
\newcommand{\lv}[0]{\ensuremath{\boldsymbol{l}}\xspace}
\newcommand{\mv}[0]{\ensuremath{\boldsymbol{m}}\xspace}
\newcommand{\nv}[0]{\ensuremath{\boldsymbol{n}}\xspace}
\newcommand{\ov}[0]{\ensuremath{\boldsymbol{o}}\xspace}
\newcommand{\pv}[0]{\ensuremath{\boldsymbol{p}}\xspace}
\newcommand{\qv}[0]{\ensuremath{\boldsymbol{q}}\xspace}
\newcommand{\rv}[0]{\ensuremath{\boldsymbol{r}}\xspace}
\newcommand{\sv}[0]{\ensuremath{\boldsymbol{s}}\xspace}
\newcommand{\tv}[0]{\ensuremath{\boldsymbol{t}}\xspace}
\newcommand{\uv}[0]{\ensuremath{\boldsymbol{u}}\xspace}
\newcommand{\vv}[0]{\ensuremath{\boldsymbol{v}}\xspace}
\newcommand{\wv}[0]{\ensuremath{\boldsymbol{w}}\xspace}
\newcommand{\xv}[0]{\ensuremath{\boldsymbol{x}}\xspace}
\newcommand{\yv}[0]{\ensuremath{\boldsymbol{y}}\xspace}
\newcommand{\zv}[0]{\ensuremath{\boldsymbol{z}}\xspace}

\newcommand{\Gammamat}[0]{\ensuremath{\boldsymbol{\Gamma}}\xspace}
\newcommand{\Deltamat}[0]{\ensuremath{\boldsymbol{\Delta}}\xspace}
\newcommand{\Thetamat}[0]{\ensuremath{\boldsymbol{\Theta}}\xspace}
\newcommand{\Lambdamat}[0]{\ensuremath{\boldsymbol{\Lambda}}\xspace}
\newcommand{\Ximat}[0]{\ensuremath{\boldsymbol{\Xi}}\xspace}
\newcommand{\Pimat}[0]{\ensuremath{\boldsymbol{\Pi}}\xspace}
\newcommand{\Sigmamat}[0]{\ensuremath{\boldsymbol{\Sigma}}\xspace}
\newcommand{\Upsilonmat}[0]{\ensuremath{\boldsymbol{\Upsilon}}\xspace}
\newcommand{\Phimat}[0]{\ensuremath{\boldsymbol{\Phi}}\xspace}
\newcommand{\Psimat}[0]{\ensuremath{\boldsymbol{\Psi}}\xspace}
\newcommand{\Omegamat}[0]{\ensuremath{\boldsymbol{\Omega}}\xspace}

\newcommand{\alphav}[0]{\ensuremath{\boldsymbol{\alpha}}\xspace}
\newcommand{\betav}[0]{\ensuremath{\boldsymbol{\beta}}\xspace}
\newcommand{\gammav}[0]{\ensuremath{\boldsymbol{\gamma}}\xspace}
\newcommand{\deltav}[0]{\ensuremath{\boldsymbol{\delta}}\xspace}
\newcommand{\epsilonv}[0]{\ensuremath{\boldsymbol{\epsilon}}\xspace}
\newcommand{\zetav}[0]{\ensuremath{\boldsymbol{\zeta}}\xspace}
\newcommand{\etav}[0]{\ensuremath{\boldsymbol{\eta}}\xspace}
\newcommand{\thetav}[0]{\ensuremath{\boldsymbol{\theta}}\xspace}
\newcommand{\iotav}[0]{\ensuremath{\boldsymbol{\iota}}\xspace}
\newcommand{\kappav}[0]{\ensuremath{\boldsymbol{\kappa}}\xspace}
\newcommand{\lambdav}[0]{\ensuremath{\boldsymbol{\lambda}}\xspace}
\newcommand{\muv}[0]{\ensuremath{\boldsymbol{\mu}}\xspace}
\newcommand{\nuv}[0]{\ensuremath{\boldsymbol{\nu}}\xspace}
\newcommand{\xiv}[0]{\ensuremath{\boldsymbol{\xi}}\xspace}
\newcommand{\omicronv}[0]{\ensuremath{\boldsymbol{\omicron}}\xspace}
\newcommand{\piv}[0]{\ensuremath{\boldsymbol{\pi}}\xspace}
\newcommand{\rhov}[0]{\ensuremath{\boldsymbol{\rho}}\xspace}
\newcommand{\sigmav}[0]{\ensuremath{\boldsymbol{\sigma}}\xspace}
\newcommand{\tauv}[0]{\ensuremath{\boldsymbol{\tau}}\xspace}
\newcommand{\upsilonv}[0]{\ensuremath{\boldsymbol{\upsilon}}\xspace}
\newcommand{\phiv}[0]{\ensuremath{\boldsymbol{\phi}}\xspace}
\newcommand{\chiv}[0]{\ensuremath{\boldsymbol{\chi}}\xspace}
\newcommand{\psiv}[0]{\ensuremath{\boldsymbol{\psi}}\xspace}
\newcommand{\omegav}[0]{\ensuremath{\boldsymbol{\omega}}\xspace}

\newcommand{\varepsilonv}[0]{\ensuremath{\boldsymbol{\varepsilon}}\xspace}
\newcommand{\varthetav}[0]{\ensuremath{\boldsymbol{\vartheta}}\xspace}
\newcommand{\varpiv}[0]{\ensuremath{\boldsymbol{\varpi}}\xspace}
\newcommand{\varrhov}[0]{\ensuremath{\boldsymbol{\varrho}}\xspace}
\newcommand{\varsigmav}[0]{\ensuremath{\boldsymbol{\varsigma}}\xspace}
\newcommand{\varphiv}[0]{\ensuremath{\boldsymbol{\varphi}}\xspace}

\newcommand{\eps}[0]{\ensuremath{\epsilon}\xspace}
\newcommand{\xtilde}[0]{\ensuremath{\widetilde{{\bf x}}}\xspace}
\newcommand{\xhat}[0]{\ensuremath{\widehat{\xv}}\xspace}
\newcommand{\Gauss}[3]{\mathcal{N}\left(#1|#2,#3\right)}

\newcommand{\ang}[1]{\langle{#1}\rangle}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{pADMM}

\begin{document}

\twocolumn[
\icmltitle{Probabilistic Alternating Direction Method of Multipliers}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2014
% package.
\icmlauthor{Your Name}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute,
            314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute,
            27182 Exp St., Toronto, ON M6H 2T1 CANADA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Distributed Learning, Bayesian Inference, Big Data}

\vskip 0.3in
]

\begin{abstract} 

\end{abstract} 

\section{Introduction}

And in sum, the main contribution of this work is that we formulated the distributed learning problem into a series of probabilistic frameworks in Section \ref{proposed_frameworks}. And importantly, with the proposed generalized Expectation-Maximization (gEM) algorithm, we show that the probabilistic methods share similar computational merits of their well-studied non-probabilistic counterparts in the large-scale setting.

\section{Preliminaries}
\subsection{Problem formulation}
In the following we consider a general regularized empirical risk minimization problem of the form $\min_\theta ~ \sum_{n\in\Omega}l(y_n; \theta)+ h(\theta)$, here $\theta$ is the parameter of interest, which could be a vector or a matrix; $l(y_n; \theta)$ denotes the loss function, which could be non-convex; $h(\theta)$ denotes the regularization function, which could be non-smooth; and $\mathcal{Y} = \{y_n, n\in\Omega\}$ denotes the observed dataset with $y_n$ representing the $n^{th}$ instance and $\Omega$ representing the dataset's index set. In a large-scale setting, the size of the dataset $|\mathcal{Y}|$ becomes too large for a single machine to store all of the data, or at least to keep the data in memory. In this work we focus on the case when $\mathcal{Y}$ is stored distributedly by a cluster of computing nodes connected through network. More concretely, we assume $\mathcal{Y}$ is partitioned into $B$ blocks, where $B$ may equal to the number of computing nodes in a cluster, and we denote each block as $\mathcal{Y}_b = \{y_n, n \in \Omega_b\}$ with $\Omega_b$ representing the $b^{th}$ data block's index set. In the distributed setting, the regularized empirical risk minimization problem can be formulated as
\begin{equation}\label{general_obj}
\textstyle\min_{\theta} f(\theta) = \sum_{b=1}^Bl(\mathcal{Y}_b; \theta)+h(\theta)
\end{equation}
where $l(\mathcal{Y}_b; \theta) = \sum_{n\in\Omega_b}l(y_n; \theta)$ is the loss function for data block $b$. In the following we will focus on distributed learning algorithms solving (\ref{general_obj}), and $\hat\theta = \argmin_\theta f(\theta)$ will be used to represent the (local) optimal solution.

\subsection{Average mixture based algorithms}

One of the well studied distributed learning algorithms for (\ref{general_obj}) is the average mixture (AVGM) method \cite{Mann2009, Zinkevich2010, YZhang2012}. The basic idea of AVGM is to augment a local parameter $\theta_b$ for each block $b$, and the estimate of $\theta_b$, denoted as $\hat\theta_b$, is computed based on $\mathcal{Y}_b$. Finally the estimate of global parameter $\theta$ is obtained by taking a (weighted) average over local estimates$\hat\theta_b$. Mathematically AVGM can be summarized as
\begin{equation}\label{avgm}
\hat\theta_b = \textstyle\argmin_{\theta_b}f(\theta_b), \quad \hat\theta = \textstyle\sum_{b=1}^B w_b\hat\theta_b/\sum_{b=1}^Bw_b
\end{equation} 
here $f(\theta_b)$ is the local objective function
\begin{equation}\label{avgm_local}
f(\theta_b) = l(\mathcal{Y}_b; \theta_b)+\frac{1}{B}h(\theta_b)
\end{equation}
and $w_b$ is some weight parameter commonly set to be $1/B$ \cite{Mann2009, Zinkevich2010} or proportional to $|\mathcal{Y}_n|$ in \cite{YZhang2012}. The AVGM is one kind of the "divide-and-conquer" methods, where the data are divided into blocks with local estimate computed for each data block, then the global estimate is obtained from a consensus of the local estimates. In the following we will focus on developing methods with similar spirit.

\section{Probabilistic Frameworks for Distributed Learning}\label{proposed_frameworks}

In the AVGM type of algorithm, to obtain a quality empirical minimizer $\hat\theta$ one needs enough information in each data block $b$ to estimate $\theta_b$ as demonstrated in \cite{YZhang2012}, however, almost by definition models that require large-scale dataset will have at least subsets of local parameters $\theta_b$ that cannot be accurately estimated only based on $\mathcal{Y}_b$. Besides, the partition of $\mathcal{Y}$ into blocks $\{\mathcal{Y}_1, \ldots, \mathcal{Y}_B\}$ may also introduce extra uncertainty and bias within each block that need to be considered when designing a distributed learning algorithm. 

In this work we propose a probabilistic approach to address the aforementioned problems. The core idea is that instead of finding a point estimate $\theta_b$ for each block $b$, we consider a more general problem where we are interested in finding a distribution $q(\theta_b)$ over all possible configurations of the local parameters $\theta_b$. In the following we will first develop a series of frameworks based this idea and then discuss the corresponding inference problems.

\subsection{From a maximum entropy perspective}\label{MEM}
Perhaps the most straightforward way of formulating a empirical risk minimization problem with respect to a distribution is via the maximum entropy principle, where the problem becomes finding a distribution $\hat q(\theta_b)$, such that ($i$) the convex combination of local objective functions (\ref{avgm_local}) of block $b$ with respect to $\hat q(\theta_b)$, which is denoted as $\mathbb{E}_{\hat q(\theta_b)}[f(\theta_b)] = \int f(\theta_b)\hat q(\theta_b) d\theta_b$, is minimized, and ($ii$) the entropy of $\hat q(\theta_b)$, which is denoted as $\mathbb{H}[\hat q(\theta_b)] = -\int \hat q(\theta_b)\log  \hat q(\theta_b) d\theta$, is maximized. After obtaining $\hat q(\theta_b)$, $\hat\theta$ is estimated as a (weighted) average of the mean of $\hat q(\theta_b)$. Mathematically, the problem is formulated as:
\begin{equation}\label{mem}
\begin{gathered}
\textstyle\hat q(\theta_b) =\argmin_{q(\theta_b)} -\mathbb{H}[q(\theta_b)] +  \mathbb{E}_{q(\theta_b) }[f(\theta_b)]\\
\hat\theta = \textstyle\sum_{b=1}^Bw_b\mathbb{E}_{\hat q(\theta_b) }[\theta_b]/\sum_{b=1}^Bw_b
\end{gathered}
\end{equation}
In the following we refer (\ref{mem}) as Maximum Entropy Mixture (MEM) method. The intuitive meaning of (\ref{mem}) can be understood as by maximizing the entropy (minimizing the negative entropy) of $q(\theta_b)$, $q(\theta_b)$ is chosen to be the distribution that makes the least claim to being informed beyond minimizing the convex combination of local objective functions $\mathbb{E}_{q(\theta_b) }[f(\theta_b)]$. The maximum entropy principle is widely used in natural language processing literature. Note that based on the estimated set of distributions $\{\hat q(\theta_1), \ldots, \hat q(\theta_B)\}$, we can assess the quality of the estimator $\hat\theta$, \eg calculating the credible/confidence interval of $\hat\theta$ as discussed in \cite{Kleiner2012}, but in order to relate our method to the AVGM method, for the MEM method we only consider the case where the point estimate of $\hat\theta$ only of interest, which is taken to be a (weighted) average of the mean of the parameters $\theta_b$ as in (\ref{mem}).

It can be shown that the optimal solution for (\ref{mem}) has the following simple form
\begin{equation}\label{mem_solution}
\hat q(\theta_b) = \frac{1}{Z}e^{-f(\theta_b)}
\end{equation}
here $Z = \int e^{-f(\theta_b)} d\theta_b$ is the normalization constant. Note that when $\hat q(\theta_b)$ is constrained to be a Dirac delta distribution with point mass at $\hat\theta_b$, \eg $\hat q(\theta_b) = \delta_{\hat\theta_b}(\theta_b)$, some simple algebra will show that $\hat\theta_b$ corresponds to the empirical local minimizer of AVGM method in (\ref{avgm}), which suggests that the AVGM method based solution is a special case of the MEM method. Another way to look at it is that the empirical minimizer of AVGM in (\ref{avgm}) is actually the maximum likelihood estimator of distribution $\hat q(\theta_b)$ in (\ref{mem_solution}).

\subsection{From a hierarchical modelling perspective}\label{HM}

One common feature of the AVGM and MEM methods is that, since the whole dataset $\mathcal{Y}$ is partitioned into blocks at different computing nodes, each computing node $b$ computes the local estimate only uses data in the block $b$, \ie only depends on $\mathcal{Y}_b$, and is independent of data from other blocks. Although this strategy leads to communication-efficient algorithms as different computing nodes only needs to communicates once \cite{YZhang2012}, as discussed in the beginning of Section \ref{proposed_frameworks} that the information contained in a single block may not be sufficient to obtain accurate local estimators, it may be helpful to share the statistical strength of data blocks among different computing nodes, while not increasing too much communication cost among the computing nodes.

%Is Bayesian hierarchical model particularly good at sharing statistics? Can we say something about it as the motivation to use it?

In the next we propose a method that facilitates the sharing of statistical strength among data blocks of different computing nodes in a distributed setting, through hierarchical modelling and Bayesian integration of prior information. In this framework we model $\theta_b$ as latent (unobserved) random variable, with prior distribution $p(\theta_b | \theta)$, \ie the local latent variable $\theta_b$ is dependent on the global parameter $\theta$ \emph{a priori}, and the latent random variables $\theta_b$ is conditionally independent of each other given $\theta$. Similar to MEM, we are interested in finding a distribution $q(\theta_b)$ over the latent random variable $\theta_b$ by minimizing a convex combination of local objective functions, but different than MEM here we make additional assumption on $q(\theta_b)$, such that the distance between $q(\theta_b)$ and the prior distribution $p(\theta_b|\theta)$ is minimized, with the distance between distributions measured by the Kullback-Leibler divergence.  The proposed hierarchical model (referred to as HM in the following sections) can be formulated as follows
\begin{equation}\label{hm}
\textstyle \min_{\theta, q(\cdot)} ~ \sum_{b=1}^B f(q(\theta_b), \theta) + h(\theta)
\end{equation}
where $q(\cdot) = \{q(\theta_1), \ldots, q(\theta_B)\}$ and the local objective function defined as
\begin{equation}\label{local_obj}
f(q(\theta_b), \theta)  = \mathbb{D}[q(\theta_b)||p(\theta_b|\theta)] + \mathbb{E}_{q(\theta_b)}[l(\mathcal{Y}_b; \theta_b)]
\end{equation}
where $\mathbb{D}[q(\theta_b)||p(\theta_b|\theta)] = \int q(\theta_b)\log\frac{q(\theta_b)}{p(\theta_b|\theta)}d\theta_b$ denotes the Kullback-Leibler (KL) divergence between $q(\theta_b)$ and $p(\theta_b|\theta)$. We can recover the previous MEM method by placing an uninformative (possibly improper) prior on $\theta_b$. 

Note that through the introduced hierarchy between data block $\mathcal{Y}_b$, latent random variable $\theta_b$ and global parameter $\theta$, the HM method maintains the independence among different data blocks, \ie they still can be stored in a distributed fashion, while facilitating the sharing of statistical strength among latent random variables in different blocks, as they are dependent on the same global parameter $\theta$ through prior distribution $p(\theta_b|\theta)$. Besides, the formulation of HM naturally leads us to a generalized Expectation-Maximization (gEM) algorithm to estimate $\theta$ and $q(\theta_b)$ iteratively within the same framework, instead of two separate steps as in AVGM and MEM, as we discuss in the following.
%is this a *principled* EM algorithm here, as we may not have the likelihood function (when l(Y_b|\theta) is not a likelihood function) ?
%Can we use the exponential family as an example to better illustrate the gEM algorithm?
The gEM algorithm proceeds by iteratively applying two steps, the E-step finds a distribution $q(\theta_b)$ that minimizes objective function (\ref{hm}) given a fixed $\theta$, and the M-steps finds an estimator $\theta$ that minimizes (\ref{hm}), based on the integrations (\ie the expectation and KL-divergence) in (\ref{hm}) calculated via $q(\theta_b)$ returned from the E-step. More concretely in the E-step we solves the optimization problem
$
q^{k+1}(\theta_b) = \argmin_{q(\theta_b)}~ f(q(\theta_b), \theta^k)
$
, where the superscript $k$ indexes the iteration, and $q^{k+1}(\theta_b)$ can be shown has the following optimal solution: 
\begin{equation}\label{hm_e_step_sol}
q^{k+1}(\theta_b) = \frac{1}{Z}p(\theta_b|\theta^k)e^{-l(\mathcal{Y}_b; \theta_b)}
\end{equation}
where $Z = \int_{\theta_b}p(\theta_b|\theta^k)e^{-l(\mathcal{Y}_b; \theta_b)}d\theta_b$ is the normalization constant. 

%am I now stating the obvious?

Before we move on to discuss the M-step, note that minimizing objective function (\ref{local_obj}) with respect to $q(\theta_b)$ is closely related to the Bayes theorem: $p(\theta_b|\mathcal{Y}_b, \theta^k) = \frac{p(\theta_b|\theta^k)p(\mathcal{Y}_b|\theta_b)}{\int p(\theta_b|\theta^k)p(\mathcal{Y}_b|\theta_b)d\theta_b}$ with $p(\theta_b|\theta^k)$ and $p(\mathcal{Y}_b|\theta_b)$ representing the prior distribution density and likelihood function respectively. Zellner \yrcite{Zellner1988} shows that the Bayesian posterior density $p(\theta_b|\mathcal{Y}_b, \theta^k)$ can equivalently be found by solving the following minimization problem:
\begin{equation}\label{bayes_rule}
\textstyle \min_{q(\theta_b)} ~\mathbb{D}[q(\theta_b)||p(\theta_b|\theta^k)] + \mathbb{E}_{q(\theta_b)}[-\log p(\mathcal{Y}_b|\theta_b)]
\end{equation}
By comparing (\ref{local_obj}) against (\ref{bayes_rule}), we see that if $p(\mathcal{Y}_b|\theta_b) \propto e^{-l(\mathcal{Y}_b; \theta_b)}$, \ie when $l(\mathcal{Y}_b; \theta_b)$ can be interpreted as a negtive log-likelihood function, then $q^{k+1}(\theta_b)$ in (\ref{hm_e_step_sol}) is exactly the posterior distribution $p(\theta_b|\mathcal{Y}_b, \theta^k)$, \ie $q^{k+1}(\theta_b) = p(\theta_b|\mathcal{Y}_b, \theta^k)$. In this case, the E-step in our gEM method reduces to the E-step in the classic EM algorithms. In many scenarios the loss function $l(\mathcal{Y}_b; \theta_b)$ does have a probabilistic interpretation, \eg  squared $\ell_2$ and $\ell_1$ loss functions are proportional to the  Gaussian and Laplace negative log-likelihood function respectively, however, some important loss functions doesn't have a probabilistic counterpart, \eg the hinge loss function for max-margin methods. As a result, in the following we will call the proposed algorithm gEM, as it can be applied to a broader class of models. 

Now given $q^{k+1}(\theta_b)$, the M-step consists of updating $\theta^{k+1}$ by solving the following problem
\begin{equation}\label{hm_m_step}
\min_{\theta}~\textstyle\sum_{b=1}^B\mathbb{E}_{q^{k+1}(\theta_b)}[-\log p(\theta_b|\theta)] + h(\theta)
\end{equation}
In sum, the proposed gEM algorithm iterates over the following two steps, ($i$) each computing node estimates distribution $q^{k+1}(\theta_b)$ by (\ref{hm_e_step_sol}) independently given the global parameter $\theta^k$, and ($ii$) the information about distribution $q^{k+1}(\theta_b)$ are gathered to estimate the global parameter by solving (\ref{hm_m_step}), and then scatters the updated $\theta^{k+1}$ to each computing node to refine their estimate on $q^{k+2}(\theta_b)$.

Related framework of (\ref{hm}) can be found in the literature known as the minimum relative entropy method \cite{Jaakkola1999, Zhu2012} for discriminative learning tasks, or the posterior regularization method \cite{Ganchev2010} for incorporating constraint to posterior inference and low rank multi-task learning \cite{Koyejo2013}.

\subsection{Small-sample bias problem}\label{pADMM}

As discussed in the beginning of Section \ref{proposed_frameworks}, in the large-scale setting $\mathcal{Y}$ is partitioned into blocks $\{\mathcal{Y}_1, \ldots, \mathcal{Y}_B\}$, and consequently the bias that is absent in $\mathcal{Y}$ may present in some of the blocks $\mathcal{Y}_b$, which may lead to biased local estimator. This so-called small-sample bias problem is studied in \cite{YZhang2012, Scott2013}, which has shown that the small-sample bias is a critical issue that affects the performance of distributed learning algorithms especially with the increasing number of partitions $B$. Such problem is addressed with the bootstrap/jackknife bias correction method in the AVGM setting as a post-processing step \cite{YZhang2012, Scott2013}, where the local estimate needs to be shifted to avoid an accumulation of small sample biases in the global estimate. And similar idea can be straightforwardly applied to the MEM method discussed in Section \ref{MEM}.

In this section we study the small-sample bias problem by explicitly model it as an integrated part of the HM method discussed in Section \ref{HM}. As we will show later that such consideration will empirically reduce the number of iterations needed to reach a (local) optimal solution, which is important for distributed learning algorithms as less number of iterations means less communications between the computing nodes. The basic idea of our method is to constrain the mean of local distribution $q(\theta_b)$ to be consistent with each other, while only allowing the curvature to vary across different blocks. Mathematically this idea can be formulated as follows 
\begin{equation}\label{padmm}
\begin{gathered}
\min_{\theta, q(\cdot)} \quad\textstyle\sum_{b=1}^B f(q(\theta_b), \theta) + h(\theta)\\
\st \quad \mathbb{E}_{q(\theta_b)}[\theta_b] = \theta, ~ b = 1, \ldots, B
\end{gathered}
\end{equation}
where $f(q(\theta_b), \theta)$ is the local objective function defined same as in (\ref{local_obj}). We name this method pADMM, which stands for probabilistic Alternating Direction Method of Multipliers, as we will show later that (\ref{padmm}) is closely related to the Alternating Direction Method of Multipliers (ADMM), which is gathering much attentions in a variety of applications in recent years, and a comprehensive survey on ADMM, especially its application on distributed learning, can be found in \cite{Boyd10}. In order to make the link of (\ref{padmm}) and ADMM more transparent, in the following we will focus on discussing a concrete case
\begin{equation}\label{gaussian_prior}
p(\theta_b|\theta) = \mathcal{N}(\theta_b | \theta, \gamma_b^{-1}\Imat)
\end{equation}
such that the prior distribution is Gaussian with mean $\theta$ and isotropic covariance matrix $\gamma_b^{-1}\Imat$, where $\Imat$ is the identity matrix of size $P\times P$ with $P$ denotes the dimension of parameter $\theta$ and $\theta_b$. In the following $\gamma_b$ will be referred to as precision parameter, which intuitively models the curvature of the prior distribution. Note that we also could model the precision parameter as a global parameter, \eg $\gamma_b = \gamma, \forall b$ as well, however, we wish to allow each block to has the freedom to express it's own characteristic by having different precision parameters, and empirically we found such block-specific precision parameters will lead to faster convergence of the algorithm, and thus less communications between computing nodes.

In the next we apply the gEM algorithm discussed in Section \ref{HM} to pADMM. Based on the Gaussian assumption in (\ref{gaussian_prior}), the local objective function $f(q(\theta_b), \theta)$ in (\ref{padmm}) becomes (with constants independent of $q(\theta_b)$ and $\theta$ ignored)
\begin{equation}\label{local_objective_gaussian}
f(q(\theta_b), \theta) = \mathbb{E}_{q(\theta_b)}\left[\frac{\gamma_b}{2}||\theta_b - \theta||_2^2 + l(\mathcal{Y}_b; \theta_b)\right]
\end{equation}
In the E-step, given the global parameter estimator $\hat\theta$, we have the Lagrangian for each local optimization problem as follows
\begin{equation}\label{padmm_lagrangian}
\begin{gathered}
\min_{q(\theta_b), \lambda_b} ~ f(q(\theta_b), \theta) + \lambda_b\cdot(\mathbb{E}_{q(\theta_b)}[\theta_b]- \theta)
\end{gathered}
\end{equation}
Where $f(q(\theta_b), \theta)$ is defined in (\ref{local_objective_gaussian}) and $\lambda_b$ is the Lagrange dual variable and $\cdot$ represents the dot product. The dual of (\ref{padmm_lagrangian}) can be readily derived, and we propose to solve problem (\ref{padmm_lagrangian}) via dual ascent, \eg the dual problem is solved using gradient ascent. The updating equation of $q(\theta_b)$ and $\lambda_b$ has the following form
\begin{equation}\label{padmm_primal_sol}
q^{k+1}(\theta_b) = \frac{1}{Z}e^{-l(\mathcal{Y}_b; \theta_b) -  \frac{\gamma_b}{2}||\theta_b-\theta^k + \mu^k_b||^2_2}
\end{equation}
where $Z = \int e^{-l(\mathcal{Y}_b; \theta_b) -  \frac{\gamma_b}{2}||\theta_b-\theta^k + \mu^k_b||^2_2} d\theta_b$ is the normalization constant, and $\mu^k_b = \frac{1}{\gamma_b}\lambda^k_b$ is the scaled dual variable, which intuitively models the bias/adjustment of the local estimates, and pADMM reduces to HM by fixing $\lambda^k_b = 0$, \ie no modelling of the small-sample bias. The dual variable $\lambda_b$ is updated using gradient ascent with step size $\gamma_b$ (the motivation to choose $\gamma_b$ as step size is inspired by ADMM as we will discuss later):
\begin{equation}\label{padmm_dual_sol}
\lambda^{k+1}_b = \lambda^k_b + \gamma_b(\mathbb{E}_{q^{k+1}(\theta_b)}[\theta_b] - \theta^k)
\end{equation}
Finally, given $q^{k+1}(\theta_b)$ and $\lambda^{k+1}_b$ or the scaled version $\mu^{k+1}_b$ obtained from the E-step for each local block $b$, the M-step updates $\theta^{k+1}$ by solving the following optimization problem
\begin{equation}\label{padmm_m_step}
\min_{\theta} ~\textstyle\sum_{b=1}^B\frac{\gamma_b}{2}||\theta-\mathbb{E}_{q^{k+1}(\theta_b)}[\theta_b] + \mu^{k+1}_b||^2_2 + h(\theta)
\end{equation}
which can be solved by proximal gradient methods \cite{Parikh13}. As one example, when $h(\theta) = \beta(\alpha||\theta||_1 + (1-\alpha)||\theta||^2_2)$ corresponds the elastic-net regularizer with parameter $0\le\alpha\le 1$ and $\beta\ge0$, the updating equation for $\theta^{k+1}$ is
\begin{equation}\label{elastic_net}
\theta^{k+1} = \frac{S\left(\textstyle\sum_{b=1}^B\gamma_b\mathbb{E}_{q^{k+1}}[\theta_b], \beta\alpha \right)}{\textstyle\sum_{b=1}^B\gamma_b + \beta(1-\alpha)}
\end{equation}
Where $S(a, b) = \mbox{sign}(a)(|a|-b)_{+}$ is the soft thresholding operator \cite{Friedman2010}. Note that (\ref{elastic_net}) resembles the weighted average strategy in (\ref{avgm}), however, instead of fixing $w_b$ to $1/B$ or proportional to $|\Omega_b|$, the weight corresponds to the precision parameter $\gamma_b$. We also propose to update $\gamma_b$ in the M-step by minimizing (\ref{padmm}) with respect to $\gamma_b$, which leads to the following updating equation for the reciprocal of $\gamma_b^{k+1}$:
\begin{equation}\label{em_bayes}
1/\gamma_b^{k+1} = \frac{1}{P}\mathbb{E}_{q^{k+1}(\theta_b)}[(\theta_b^{k+1}-\theta^{k+1}+\mu_b^{k+1})^2]
\end{equation}
Where recall that $P$ is the dimension of $\theta_b$. Note that by setting $\mu_b = 0$, (\ref{em_bayes}) reduces to the empirical Bayes estimate of $\gamma_b$ for HM under the Gaussian assumption (\ref{gaussian_prior}). As a side note, for the HM method if $\{\gamma_b^k\}$ is setting to be an increasing sequence, \eg $\gamma_b^{1} < \gamma_b^{2} < \ldots$ and letting $\gamma_b^{k}$ converges to $\infty$, while constraining $q(\theta_b)$ to be a Dirac delta function, HM reduces to a class of the penalty methods \cite{Bertsekas1989, Bertsekas1996}, which can also be used to solve distributed learning problems, although generally with slower convergence rate than ADMM \cite{Bertsekas1989}. To better understands pADMM, in the following we briefly review the well-studied and widely-used ADMM method, and demonstrates how pADMM relates to ADMM.

% I discuss ADMM here partly because ADMM will converge, and I show ADMM actually finds a point estimate of pADMM, in this way I don't need to show some convergence result for pADMM, which is difficult to me. 
\subsubsection{Distributed learning via ADMM}\label{ADMM}
Here we only summarize important aspects of ADMM, a more detailed discussion can be found at \cite{Boyd10}. First note that we can re-write (\ref{general_obj}) into the following equivalent optimization problem:
\begin{equation}\label{admm_obj}
\begin{gathered}
\min_{\theta, \theta_b} ~ \textstyle\sum_{b=1}^Bl(\mathcal{Y}_b,\theta_b) + h(\theta)\\
\st ~ \theta_b - \theta = 0, ~ b = 1,\ldots,B
\end{gathered}
\end{equation}
The ADMM formulation for the problem (\ref{admm_obj}) can be derived directly from the following augmented Lagrangian
\begin{equation}\label{admm_lag_global}
L_\gamma(\{\theta_b, \lambda_b\}_{b=1}^B, \theta) =  \textstyle\sum_{b=1}^BL_\gamma(\theta_b, \lambda_b, \theta) + h(\theta)
\end{equation}
Here $L_\gamma(\theta_b, \lambda_b, \theta)$ is the local augmented Lagrangian:
\begin{equation}\label{admm_lag_local}
L_\gamma(\theta_b, \lambda_b, \theta)  = l(\mathcal{Y}_b, \theta_b) + \lambda_b\cdot(\theta_b - \theta) + \frac{\gamma}{2}||\theta_b - \theta||_2^2
\end{equation}
Where $\gamma \ge 0$ is the tuning parameter of the augmented Lagrangian. ADMM proceeds by iteratively updating $\theta_b, \lambda_b$ for each block $b$ independently and $\theta$:
\begin{equation}\label{admm_update_local}
\theta_b^{k+1} = \argmin_{\theta_b}~L_\gamma(\theta_b, \lambda_b^k, \theta^k)
\end{equation}
\begin{equation}\label{admm_update_lag}
\lambda_b^{k+1} = \lambda_b^k + \gamma(\theta_b^{k+1} - \theta^{k})
\end{equation}
\begin{equation}\label{admm_update_global}
\theta^{k+1} = \argmin_\theta ~\textstyle\sum_{b=1}^B\frac{\gamma}{2}||\theta_b^{k+1} - \theta + \mu_b^{k+1}||^2_2 + h(\theta)
\end{equation}
Note that in the gradient ascent update in (\ref{admm_update_lag}) the step size is chosen to be $\gamma$. The motivation behind this choice is explained by Boyd el al. \yrcite{Boyd10}, that by using $\gamma$ as the step size, the iterate
$\theta_b^{k+1}, \lambda_b^{k+1}$ in (\ref{admm_update_local} - \ref{admm_update_lag}) is dual feasible, and as the ADMM proceeds the primal residual $\theta_b -\theta$ converges to zero, together with the dual feasible condition the procedure (\ref{admm_update_local} - \ref{admm_update_global}) will yield optimal solution $\hat\theta$. 

Now comparing updating equations for ADMM in (\ref{admm_update_local} - \ref{admm_update_global}) against that of for pADMM (\ref{local_objective_gaussian}, \ref{padmm_primal_sol} - \ref{padmm_m_step}), we see that the gEM algorithm for pADMM can be thought as a probabilistic version of ADMM, hence the name, and ADMM can be thought as a way of finding a point estimate for pADMM, with pADMM reduces to ADMM when $q(\theta_b) = \delta_{\hat\theta_b}(\theta_b)$ becomes a Dirac delta distribution. This also resembles the relationship between AVGM and MEM. Note that carefully tuning the parameter $\gamma$ is one caveat of reaching fast convergence of ADMM, which is commonly done by cross-validation, which is particularly computationally expensive in the large-scale setting. However, as mentioned before the fast convergence is also particularly critical in the distributed learning scenario as less number of iterations means less expensive communication among computing nodes when the dimension of the parameter is high. Although the same problem exists for HM and pADMM, we will show empirically that by updating $\gamma_b$ as in (\ref{em_bayes}), pADMM mitigates the dilemma ADMM faces by having comparable or faster convergence against a well-tuned ADMM.


\subsection{Full Bayesian treatment}\label{Full_Bayesian}
Although the frameworks we proposed, \ie MEM, HM and pADMM, are motivated by the optimization problem (\ref{general_obj}), they can naturally be extended to full Bayesian frameworks. As discussed in Section \ref{HM} and equation (\ref{bayes_rule}), the gEM algorithm proposed for HM and pADMM corresponds to the EM algorithm for maximum likelihood estimation or maximum a posteriori probability estimate when $l(\mathcal{Y}_b; \theta_b)$ corresponds to a negative log-likelihood function. Besides that, if the global parameter $\theta$ also treated as a random variable and placed with a prior distribution instead of a regularization function $h(\theta)$ as in (\ref{general_obj}), such that the M-step is replaced by a Bayesian posterior inference procedure, then we have a full Bayesian model. However, this topic involves the design of efficient inference algorithm for Bayesian posterior computation and is beyond the scope of this work. \cite{Scott2013, Neiswanger2013} represent two recent work in this direction, although their frameworks are both in the AVGM based setting.
 

\section{Distributed (sub-)gradient method}
We have been focused on the "divide-and-conquer" type of frameworks thus far, \eg AVGM, MEM, HM, ADMM and pADMM, and in this section we briefly discuss another type of distributed learning algorithm, the distributed sub-gradient method, which will serve as an important baseline when we compare different distributed algorithm empirically through experiments. The distributed sub-gradient method for optimization problem (\ref{general_obj}) is straightforward, where for each iteration the sub-gradients $\partial l(\mathcal{Y}_b; \theta)$ are computed by in parallel for each block $b$, and these separate sub-gradients are then summed up to compute the exact global sub-gradients $\textstyle\sum_{b=1}^B\partial l(\mathcal{Y}_b; \theta)$, which are used to perform the optimization step and update the parameter $\theta$ received by all $B$ blocks for the next iteration's sub-gradients computation. Each iteration of the distributed sub-gradient method can be readily expressed under the MapReduce paradigm \cite{Dean2004}, however, one potential problem of the distributed sub-gradient method is that, it requires relatively many iterations to converge will requires constant communications among computing nodes in the cluster, which can be a drawback when the dimensionality of the data and the parameter is high, as communication can be prohibitively expensive. As a result, the AVGM and MEM based method represents one extreme of the distributed learning framework, where communication only happens once but the solution might not be optimal with finite number of data (although asymptotically optimal); and the distributed sub-gradient method is another extreme with frequent communications but is guaranteed to find the (local) optimal solution; at last ADMM and the proposed HM, ADMM methods lies in the middle of the two.


\section{Practical Issues on the gEM Algorithm}
In this section we first discuss some practical considerations for the proposed gEM algorithm, where one core issue in making the gEM practical in large-scale setting is to evaluate the expectations efficiently in the E-step, \eg $\mathbb{E}_{q(\theta_b)}[\theta_b]$, which in turn generally requires an analytical form of the distribution $q(\theta_b)$, or at least be able to draw samples from it. And given the expectations found in the E-step, efficient optimization algorithms, \eg the proximal gradient method or stochastic gradient based methods, becomes applicable for the M-step. Consequently, the first key technical component in our implementation is the adoption of the variational methods \cite{Wainwright2008} in the E-step, as we briefly discuss in the following.

As discussed in Section \ref{MEM}-\ref{pADMM}, for MEM, HM and pADMM the distribution $q(\theta_b)$ of interest is found by minimizing the local objective function (\ref{local_obj}), which could be problematic when the expectation $\mathbb{E}_{q(\theta_b)}[l(\mathcal{Y}_b; \theta_b)]$ has no analytical expression. Two types of variational methods, commonly referred to as local and global variational methods in the literature \cite{Bishop2006}, can be applied here. The local variational methods find an upper bound of the loss function $\tilde l(\mathcal{Y}_b; \theta_b; \xi_b) \ge  l(\mathcal{Y}_b; \theta_b)$, then the expectation of the upper bound $\mathbb{E}_{q(\theta_b)}[\tilde l(\mathcal{Y}_b; \theta_b; \xi_b)]$ is minimized to find $q(\theta_b)$, with $\xi_b$ representing some variational parameters that can be optimized to tighten the gap between $\tilde l(\mathcal{Y}_b; \theta_b; \xi_b)$ and $l(\mathcal{Y}_b; \theta_b)$. Local variational method are well studied for various of different loss functions and Bayesian models, a few examples include \cite{Zhu2012} for hinge loss function, \cite{Jaakkola2000, Khan2010} for logistic/soft-max loss function, and \cite{Khan2013, Wang2013} for general non-conjugate models. The global variational methods restricts the range of $q(\theta)$ over which the optimization is performed, \eg the mean field methods restrict $q(\theta) = \prod_iq(\theta_i)$ to take factorized forms. Mean field based variational methods can be found in many Bayesian graphical models with latent variables, where the posterior distribution is intractable \cite{Wainwright2008}. Here we propose to use mean field variational methods from a slightly different perspective, as we will discuss later that by restricting $q(\theta)$ to be fully factorized across its components, the variational methods proceeds the form of coordinate descent (CD) algorithms, which is shown to be efficient to find the (local) optimal solution in large-scale applications from the optimization viewpoint \cite{Friedman2010, Bradley2011, Yuan2012, Yu2013}. In both cases, the gEM algorithm reduces to the variational EM algorithm, and if we extend our method to be full Bayesian as discussed in Section \ref{Full_Bayesian}, gEM corresponds to the variational Bayesian inference algorithm \cite{Bishop2006, Wainwright2008}.

Although sampling based methods, \eg MCMC and importance re-sampling, generally doesn't need to evaluate the normalization constant $Z$ for $q(\theta_b)$ explicitly and can readily be applied to our framework, the reason we prefer variational method to sampling based method is two-fold, ($i$) It is generally more difficult to monitor the convergence of a sampling based method than deterministic variational method;  and ($ii$) our preliminary experiment suggests that in the high-dimensional and large-scale setting MCMC tends to take more iterations to reach a reasonable solution than variational method, and importance re-sampling tends to collapse to a single point.



\section{Empirical Study}
With the variational gEM algorithm described above, we solve two challenging problems, the non-smooth $\ell_1$-regularized logistic regression, and the non-convex low-rank matrix completion, with our proposed distributed learning methods. In the following we first introduce our experimental setting.

\subsection{Experimental setting}

MPI has no fault tolerance and doesn't scale very well to large clusters, Hadoop requires many disk read/write, which may lead to distract our attention on analysing the reason caused the different convergence behaviour of different methods, and Hadoop is too slow in iterative algorithms. (Not finished yet)

\subsection{Distributed $\ell_1$-regularized logistic regression}
In logistic regression the each training data $y_n$ contains two parts, $y_n = \{l_n, x_n\}$ where $l_n \in \{-1, +1\}$ represents the label and $x_n \in \mathbb{R}^P$ represents the feature vector with dimension $P$, and parameter $\theta\in\mathbb{R}^P$ also has the same dimension. The loss function in (\ref{general_obj}) is $l(\mathbb{Y}_b; \theta) = \sum_{n\in\Omega_b}\log(1+\exp(-l_n(x_n\cdot\theta)))$, and the regularization function is $h(\theta) = C||\theta||_1$, where $C$ is a tuning parameter controls the sparsity of the parameter $\theta$. And we assume the prior distribution $p(\theta_b|\theta)$ is Gaussian as in (\ref{gaussian_prior}) for HM and pADMM. 

In this case distribution $q(\theta_b)$ doesn't readily have an analytical form, as the normalization constant $Z$ in (\ref{mem_solution}) for MEM, (\ref{hm_e_step_sol}) for HM, and (\ref{padmm_primal_sol}) for pADMM are all intractable. And in this work we will use the Laplace variational method \cite{Wang2013} and the Bohning bound based variational method \cite{Khan2010} as they work favourably compared to other variational methods both computationally and efficaciously in our preliminary experiments.


\subsection{Distributed low-rank matrix completion}
In our notation, $\mathcal{Y} = \{y_n, n\in\Omega\}$ corresponds to observed entries of a matrix $\Ymat$ of dimension $I \times J$, $n = (i,j)$ indexes each entry, and $\Omega$ is a subset of $\{1, \ldots, I\}\otimes\{1,\ldots, J\}$ denoting the indexes of the observed entries in $\Ymat$ with $\otimes$ denoting the Cartesian product. And similarly $\theta \in \mathbb{R}^{I\times J}$ denotes the model parameters of interest. The loss function is quadratic $l(\mathcal{Y}; \theta) = \sum_{(i,j)\in\Omega}(y_{i,j} - \theta_{i,j})^2$, and the regularization functions $h(\theta)$ we consider here are the nuclear norm and the $\gamma_2$-norm (also know as the max-norm) \cite{Srebro2004}. We assume $\theta$ has rank at most $K$, in which case $\theta$ can be explicitly written as $UV'$ where $U \in \mathbb{R}^{I\times K}$ and $V \in \mathbb{R}^{J \times K}$. Such approximation transforms the low-rank matrix completion a non-convex problem, with objective function now can be written as
\begin{equation}\label{mf_obj}
\textstyle f(U,V) = \sum_{(i,j)\in\Omega}(y_{i,j} - U_{i\cdot}V_{j\cdot}')^2 + h(U) + h(V)
\end{equation}
Note that the nuclear norm and $\gamma_2$-norm on $\theta$ can be equivalent formulated in terms of $U$ and $V$, such that the nuclear norm can be written as $h(U) = ||U||_F^2 := \sum_{i}\sum_kU_{ik}^2$, and $\gamma_2$-norm can be written as $h(U) = ||U||_{2,\infty}^2 := \max_i(\sum_k U_{ik}^2)$ \cite{Recht2010}. The optimization techniques for both norms that could be used in the M-step of the gEM algorithm can be found at \cite{Salakhutdinov2007, Lee2010}. In the large-scale setting, we randomly partition the observed data $\mathcal{Y}$ into $B$ blocks similar to the two-way random partition method in \cite{Recht2011}, with each block corresponds to a sub-matrix of size approximately $\frac{I}{B_r} \times \frac{J}{B_c}$, such that $B_rB_c = B$, and with parameters $U$ and $V$ conformally partitioned into $B_r$ and $B_c$ blocks respectively. Consequently, the pADMM formulation of the low-rank matrix completion problem is
\begin{equation}\label{mf_padmm}
\begin{gathered}
\textstyle \min_{U,V, q(\cdot)}~ \sum_{b=1}^B f(U_b,V_b, U, V) + h(U) + h(V)\\
\st ~ \mathbb{E}_{q(U_b)} = U_b, ~ \mathbb{E}_{q(V_b)} = V_b \\
\end{gathered}
\end{equation}


\begin{table}\label{mf_alg_comp}
\footnotesize
\caption{Comparison of memory and network usage for two types of matrix factorization algorithms}
\begin{tabular}{c|c|c}
~  &  network per epoch & memory per core\\
\hline
ALS/CD & $\mathcal{O}( (M+N) K B)$  & $\mathcal{O}(MK+NK + \frac{|\Omega|}{B})$  \\
Proposed & $\mathcal{O}( MK B_{c} + NK B_{r})$ & $\mathcal{O}(\frac{MK}{B_{r}}+\frac{NK}{B_{c}} + \frac{|\Omega|}{B})$ \\
\end{tabular}
\end{table}


\begin{table}\label{mf_datasets}
\caption{ Statistics and parameters for each dataset}
\begin{tabular}{c|ccc}
~ & Netflix &  KDDCup2011 & Synthetic\\
\hline
M & 2,649,429 & 1,000,990 & 1,000,000\\
N & 17,770 & 624,961 & 1,000,000 \\
$|\Omega|$ & 99,072,112 & 252,800,275 & 1,983,749,510 \\
$|\bar\Omega|$ & 1,408,395 & 4,003,960 & 23,328,835  \\
$K$ & 30 & 50 & 30\\
$\lambda$ & 0.05 & 1 & 0.001
\end{tabular}
\end{table}



% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\newpage
\bibliography{icml2014}
\bibliographystyle{icml2014}

\end{document} 
